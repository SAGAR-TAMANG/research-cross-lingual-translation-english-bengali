{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# My Step-by-Step Approach\n",
    "\n",
    "1. **Prepare the Base Model:**  \n",
    "   I begin with a Bengali–English model that has been fine-tuned solely on high-quality Bengali–English parallel data.\n",
    "\n",
    "2. **Generate Synthetic Bengali–Assamese Data:**  \n",
    "   - I collect a set of Bengali monolingual sentences.  \n",
    "   - I use an LLM (e.g., ChatGPT) to automatically generate Assamese translations for these Bengali sentences.  \n",
    "   - Optionally, I review and filter these synthetic pairs to ensure quality.\n",
    "\n",
    "3. **Fine-Tune with Synthetic Data:**  \n",
    "   I further fine-tune my Bengali–English model using the synthetic Bengali–Assamese parallel data. This step trains the model to output Assamese when given Bengali input, effectively mapping Bengali representations to Assamese text.\n",
    "\n",
    "4. **Test the New Capability:**  \n",
    "   - I evaluate the fine-tuned model by feeding it Bengali input and confirming that it produces coherent Assamese outputs.  \n",
    "   - For zero-shot English-to-Assamese translation, I first translate English into Bengali (using a separate English–Bengali model) and then pass the Bengali text to my fine-tuned model.\n",
    "\n",
    "5. **Iterate as Needed:**  \n",
    "   If the results aren’t satisfactory, I refine the synthetic data quality or adjust the fine-tuning parameters.\n",
    "\n",
    "This two-stage process leverages both conventional fine-tuning and synthetic data (backtranslation-like) to indirectly achieve zero-shot English-to-Assamese translation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: **Prepare the Base Model:**  \n",
    "   I begin with a Bengali–English model that has been fine-tuned solely on high-quality Bengali–English parallel data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\TAMANG\\Documents\\GitHub\\research-cross-lingual-translation-english-bengali\\venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n",
    "from normalizer import normalize  # Install via: pip install git+https://github.com/csebuetnlp/normalizer\n",
    "\n",
    "# reference needs to be provided: https://huggingface.co/csebuetnlp/banglat5\n",
    "\n",
    "# Load the BanglaT5 model and its tokenizer\n",
    "model_name = \"csebuetnlp/banglat5\"\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input (Bengali): আমি বাংলায় কথা বলি।\n",
      "Translation (English): I speak in Bengali. \"আমি বাংলা ভাষায় কথা বলি। \"আমি বাংলায় কথা বলি।\n"
     ]
    }
   ],
   "source": [
    "# Example Bengali sentence (change this to your desired input)\n",
    "input_sentence = \"আমি বাংলায় কথা বলি।\"\n",
    "\n",
    "# Normalize and tokenize the input sentence\n",
    "normalized_text = normalize(input_sentence)\n",
    "input_ids = tokenizer(normalized_text, return_tensors=\"pt\").input_ids\n",
    "\n",
    "# Generate with beam search and adjusted repetition control\n",
    "generated_tokens = model.generate(\n",
    "    input_ids,\n",
    "    # max_length=50,\n",
    "    # num_beams=5,                 # Use beam search with 5 beams\n",
    "    # no_repeat_ngram_size=3,      # Prevent 3-gram repetitions\n",
    "    # repetition_penalty=1.2,      # Slight penalty for repetitions\n",
    "    # early_stopping=True\n",
    ")\n",
    "translation = tokenizer.batch_decode(generated_tokens, skip_special_tokens=True)[0]\n",
    "\n",
    "print(\"Input (Bengali):\", input_sentence)\n",
    "print(\"Translation (English):\", translation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Step 2: **Generate Synthetic Bengali–Assamese Data:**  \n",
    "   - I collect a set of Bengali monolingual sentences.  \n",
    "   - I use an LLM (e.g., ChatGPT) to automatically generate Assamese translations for these Bengali sentences.  \n",
    "   - Optionally, I review and filter these synthetic pairs to ensure quality.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: **Fine-Tune with Synthetic Data:**  \n",
    "   I further fine-tune my Bengali–English model using the synthetic Bengali–Assamese parallel data. This step trains the model to output Assamese when given Bengali input, effectively mapping Bengali representations to Assamese text."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: **Test the New Capability:**  \n",
    "   - I evaluate the fine-tuned model by feeding it Bengali input and confirming that it produces coherent Assamese outputs.  \n",
    "   - For zero-shot English-to-Assamese translation, I first translate English into Bengali (using a separate English–Bengali model) and then pass the Bengali text to my fine-tuned model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5: **Iterate as Needed:**  \n",
    "   If the results aren’t satisfactory, I refine the synthetic data quality or adjust the fine-tuning parameters."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
