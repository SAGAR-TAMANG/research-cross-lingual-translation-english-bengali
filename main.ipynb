{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# main.py "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "import os\n",
    "import re\n",
    "import unicodedata\n",
    "import sentencepiece as spm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_unicode(text):\n",
    "    \"\"\"\n",
    "    Normalize text using NFC Unicode normalization.\n",
    "    \"\"\"\n",
    "    return unicodedata.normalize('NFC', text)\n",
    "\n",
    "def clean_text(text, lang='en'):\n",
    "    \"\"\"\n",
    "    Clean and normalize text:\n",
    "      - Strips extra spaces.\n",
    "      - Applies Unicode normalization.\n",
    "      - Removes extra punctuation characters (customize as needed).\n",
    "      - Converts English text to lowercase.\n",
    "    \"\"\"\n",
    "    text = text.strip()\n",
    "    text = normalize_unicode(text)\n",
    "    # Replace multiple spaces with a single space\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    # Remove some unwanted punctuation (customize if needed)\n",
    "    text = re.sub(r'[“”‘’]', '', text)\n",
    "    if lang == 'en':\n",
    "        text = text.lower()\n",
    "    return text\n",
    "\n",
    "def add_language_tag(text, lang_tag):\n",
    "    \"\"\"\n",
    "    Prepend a language tag to a sentence.\n",
    "    \"\"\"\n",
    "    return f\"{lang_tag} {text}\"\n",
    "\n",
    "def preprocess_file(input_file, output_file, lang_tag, lang='en'):\n",
    "    \"\"\"\n",
    "    Reads an input file line by line, cleans each line,\n",
    "    prepends a language tag, and writes the result to output_file.\n",
    "    \"\"\"\n",
    "    lines_cleaned = []\n",
    "    with open(input_file, 'r', encoding='utf-8') as f_in:\n",
    "        lines = f_in.readlines()\n",
    "        for line in lines:\n",
    "            cleaned = clean_text(line, lang=lang)\n",
    "            tagged = add_language_tag(cleaned, lang_tag)\n",
    "            lines_cleaned.append(tagged)\n",
    "    \n",
    "    with open(output_file, 'w', encoding='utf-8') as f_out:\n",
    "        for line in lines_cleaned:\n",
    "            f_out.write(line + \"\\n\")\n",
    "    \n",
    "    print(f\"Preprocessed {input_file} -> {output_file}\")\n",
    "    return output_file\n",
    "\n",
    "def combine_files(file_list, combined_file):\n",
    "    \"\"\"\n",
    "    Combine multiple text files into one.\n",
    "    \"\"\"\n",
    "    with open(combined_file, 'w', encoding='utf-8') as f_out:\n",
    "        for file in file_list:\n",
    "            with open(file, 'r', encoding='utf-8') as f_in:\n",
    "                f_out.write(f_in.read())\n",
    "    print(f\"Combined files into {combined_file}\")\n",
    "    return combined_file\n",
    "\n",
    "def train_sentencepiece_model(input_file, model_prefix, vocab_size=32000):\n",
    "    \"\"\"\n",
    "    Train a SentencePiece model on the provided input file.\n",
    "    This generates two files: {model_prefix}.model and {model_prefix}.vocab.\n",
    "    \"\"\"\n",
    "    spm.SentencePieceTrainer.train(\n",
    "        f\"--input={input_file} --model_prefix={model_prefix} --vocab_size={vocab_size} \"\n",
    "        f\"--model_type=bpe --character_coverage=1.0\"\n",
    "    )\n",
    "    print(f\"Trained SentencePiece model: {model_prefix}.model and {model_prefix}.vocab\")\n",
    "\n",
    "def tokenize_file(input_file, output_file, model_file):\n",
    "    \"\"\"\n",
    "    Tokenize an input file using the trained SentencePiece model,\n",
    "    and write the tokenized sentences to the output file.\n",
    "    \"\"\"\n",
    "    sp = spm.SentencePieceProcessor(model_file=model_file)\n",
    "    with open(input_file, 'r', encoding='utf-8') as f_in, \\\n",
    "         open(output_file, 'w', encoding='utf-8') as f_out:\n",
    "        for line in f_in:\n",
    "            pieces = sp.encode_as_pieces(line.strip())\n",
    "            f_out.write(\" \".join(pieces) + \"\\n\")\n",
    "    print(f\"Tokenized {input_file} -> {output_file}\")\n",
    "\n",
    "def main():\n",
    "    # Define file names for your development data\n",
    "    eng_dev = 'eng.dev'\n",
    "    ben_dev = 'ben.dev'\n",
    "    \n",
    "    # Output file names for the cleaned data\n",
    "    eng_clean = 'eng_clean.txt'\n",
    "    ben_clean = 'ben_clean.txt'\n",
    "    \n",
    "    # Combine cleaned file for SentencePiece training\n",
    "    combined_file = 'combined.txt'\n",
    "    \n",
    "    # SentencePiece model settings\n",
    "    model_prefix = 'spm_model'\n",
    "    model_file = f\"{model_prefix}.model\"\n",
    "    \n",
    "    # Output file names for tokenized data\n",
    "    eng_tokenized = 'eng_tokenized.txt'\n",
    "    ben_tokenized = 'ben_tokenized.txt'\n",
    "    \n",
    "    # 1. Preprocess each file (clean text and add language tags)\n",
    "    preprocess_file(eng_dev, eng_clean, '<en>', lang='en')\n",
    "    preprocess_file(ben_dev, ben_clean, '<bn>', lang='bn')\n",
    "    \n",
    "    # 2. Combine the cleaned files into one for training the tokenizer\n",
    "    combine_files([eng_clean, ben_clean], combined_file)\n",
    "    \n",
    "    # 3. Train the SentencePiece model on the combined data\n",
    "    train_sentencepiece_model(combined_file, model_prefix, vocab_size=32000)\n",
    "    \n",
    "    # 4. Tokenize the cleaned files using the trained SentencePiece model\n",
    "    tokenize_file(eng_clean, eng_tokenized, model_file)\n",
    "    tokenize_file(ben_clean, ben_tokenized, model_file)\n",
    "    \n",
    "    print(\"Preprocessing and tokenization complete.\")\n",
    "\n",
    "# if __name__ == '__main__':\n",
    "#     main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessed eng.dev -> eng_clean.txt\n",
      "Preprocessed ben.dev -> ben_clean.txt\n",
      "Combined files into combined.txt\n",
      "Trained SentencePiece model: spm_model.model and spm_model.vocab\n",
      "Tokenized eng_clean.txt -> eng_tokenized.txt\n",
      "Tokenized ben_clean.txt -> ben_tokenized.txt\n",
      "Preprocessing and tokenization complete.\n"
     ]
    }
   ],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# lexicon_generator.py\n",
    "\n",
    "- It reads parallel sentences from the two files.\n",
    "- It tokenizes the sentences by splitting on whitespace (you may later improve tokenization using a more sophisticated tokenizer).\n",
    "- It counts how often each English word appears and how often each English word co-occurs with each Bengali word in the same sentence pair.\n",
    "- For each English word that appears frequently enough (above a threshold), it selects the Bengali word with the highest conditional probability (i.e. highest relative co-occurrence frequency) as its candidate translation.\n",
    "- Finally, it writes the resulting lexicon (as English–Bengali pairs) to an output file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building co-occurrence counts from FLORES 101 data...\n",
      "Choosing candidate translations based on conditional probabilities...\n",
      "Generated lexicon with 520 entries.\n",
      "Lexicon saved to auto_lexicon.txt\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "import collections\n",
    "import math\n",
    "\n",
    "def tokenize(text):\n",
    "    \"\"\"\n",
    "    Simple whitespace tokenization.\n",
    "    You can later replace this with a more sophisticated tokenizer if needed.\n",
    "    \"\"\"\n",
    "    return text.strip().split()\n",
    "\n",
    "def build_cooccurrence(eng_file, ben_file):\n",
    "    \"\"\"\n",
    "    Reads parallel sentences from eng_file and ben_file,\n",
    "    and builds frequency counts and co-occurrence counts.\n",
    "    Returns:\n",
    "      eng_counts: Counter for English word frequencies.\n",
    "      cooc_counts: Dictionary mapping an English word to a Counter of Bengali words.\n",
    "    \"\"\"\n",
    "    eng_counts = collections.Counter()\n",
    "    cooc_counts = {}  # eng_word -> Counter({ben_word: count})\n",
    "    \n",
    "    with open(eng_file, 'r', encoding='utf-8') as ef, open(ben_file, 'r', encoding='utf-8') as bf:\n",
    "        eng_lines = ef.readlines()\n",
    "        ben_lines = bf.readlines()\n",
    "    \n",
    "    if len(eng_lines) != len(ben_lines):\n",
    "        raise ValueError(\"The number of lines in the English and Bengali files must be equal.\")\n",
    "    \n",
    "    for eng_line, ben_line in zip(eng_lines, ben_lines):\n",
    "        eng_tokens = tokenize(eng_line)\n",
    "        ben_tokens = tokenize(ben_line)\n",
    "        \n",
    "        # Update counts for each English token in this sentence\n",
    "        for e in eng_tokens:\n",
    "            eng_counts[e] += 1\n",
    "            if e not in cooc_counts:\n",
    "                cooc_counts[e] = collections.Counter()\n",
    "            # Count all Bengali tokens as co-occurring with this English token\n",
    "            for b in ben_tokens:\n",
    "                cooc_counts[e][b] += 1\n",
    "                \n",
    "    return eng_counts, cooc_counts\n",
    "\n",
    "def choose_translation(eng_counts, cooc_counts, min_count=5):\n",
    "    \"\"\"\n",
    "    For each English word with frequency >= min_count, choose the Bengali word\n",
    "    that maximizes the conditional probability P(b|e) = count(e,b)/count(e).\n",
    "    Returns a dictionary mapping English word to candidate Bengali translation.\n",
    "    \"\"\"\n",
    "    lexicon = {}\n",
    "    for e, freq in eng_counts.items():\n",
    "        if freq < min_count:\n",
    "            continue  # skip rare words\n",
    "        best_b = None\n",
    "        best_prob = 0.0\n",
    "        for b, cooc in cooc_counts[e].items():\n",
    "            prob = cooc / freq  # conditional probability P(b|e)\n",
    "            if prob > best_prob:\n",
    "                best_prob = prob\n",
    "                best_b = b\n",
    "        if best_b is not None:\n",
    "            lexicon[e] = best_b\n",
    "    return lexicon\n",
    "\n",
    "def save_lexicon(lexicon, output_file):\n",
    "    \"\"\"\n",
    "    Writes the lexicon to output_file, one pair per line, tab-separated.\n",
    "    \"\"\"\n",
    "    with open(output_file, 'w', encoding='utf-8') as f:\n",
    "        for e, b in lexicon.items():\n",
    "            f.write(f\"{e}\\t{b}\\n\")\n",
    "    print(f\"Lexicon saved to {output_file}\")\n",
    "\n",
    "def main():\n",
    "    eng_file = \"eng.dev\"\n",
    "    ben_file = \"ben.dev\"\n",
    "    output_file = \"auto_lexicon.txt\"\n",
    "    min_count = 5  # You can adjust this threshold based on your data\n",
    "    \n",
    "    print(\"Building co-occurrence counts from FLORES 101 data...\")\n",
    "    eng_counts, cooc_counts = build_cooccurrence(eng_file, ben_file)\n",
    "    \n",
    "    print(\"Choosing candidate translations based on conditional probabilities...\")\n",
    "    lexicon = choose_translation(eng_counts, cooc_counts, min_count=min_count)\n",
    "    \n",
    "    print(f\"Generated lexicon with {len(lexicon)} entries.\")\n",
    "    save_lexicon(lexicon, output_file)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The generated lexicon sucks, we would need to go manual, or api method."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# align_embeddings.py\n",
    "Alignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "import fasttext\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from numpy.linalg import norm, svd\n",
    "\n",
    "def load_dictionary(dict_path):\n",
    "    \"\"\"\n",
    "    Load a bilingual dictionary from a file.\n",
    "    Each line should contain an English word and its Bengali translation separated by a tab.\n",
    "    Multi-word translations are preserved.\n",
    "    Returns a list of (english, bengali) pairs.\n",
    "    \"\"\"\n",
    "    dictionary = []\n",
    "    with open(dict_path, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            parts = line.strip().split('\\t', 1)\n",
    "            if len(parts) != 2:\n",
    "                continue  # Skip malformed lines\n",
    "            en_word = parts[0].strip()\n",
    "            bn_word = parts[1].strip()\n",
    "            dictionary.append((en_word, bn_word))\n",
    "    return dictionary\n",
    "\n",
    "def get_embedding_matrix(model, words):\n",
    "    \"\"\"\n",
    "    For a list of words, return a matrix (numpy array) where each row is the embedding of that word.\n",
    "    \"\"\"\n",
    "    embeddings = []\n",
    "    for word in words:\n",
    "        vector = model.get_word_vector(word)\n",
    "        embeddings.append(vector)\n",
    "    return np.array(embeddings)\n",
    "\n",
    "def align_embeddings(src_model, tgt_model, dictionary):\n",
    "    \"\"\"\n",
    "    Align embeddings from the source (English) to target (Bengali) space using Procrustes analysis.\n",
    "    Returns the transformation matrix and SVD components for visualization.\n",
    "    \"\"\"\n",
    "    src_words = [pair[0] for pair in dictionary]\n",
    "    tgt_words = [pair[1] for pair in dictionary]\n",
    "    \n",
    "    X = get_embedding_matrix(src_model, src_words)  # English embeddings\n",
    "    Y = get_embedding_matrix(tgt_model, tgt_words)  # Bengali embeddings\n",
    "    \n",
    "    M = np.dot(Y.T, X)  # Cross-covariance matrix\n",
    "    U, S, Vt = svd(M)\n",
    "    W = np.dot(U, Vt)\n",
    "    return W, U, S, Vt\n",
    "\n",
    "def cosine_similarity(a, b):\n",
    "    return np.dot(a, b) / (norm(a) * norm(b) + 1e-8)\n",
    "\n",
    "def visualize_alignment(src_model, tgt_model, dictionary, W, output_dir=\"visualizations\"):\n",
    "    \"\"\"\n",
    "    Visualize the alignment by plotting a histogram of cosine similarities between \n",
    "    transformed English embeddings and Bengali embeddings.\n",
    "    \"\"\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    src_words = [pair[0] for pair in dictionary]\n",
    "    tgt_words = [pair[1] for pair in dictionary]\n",
    "    X = get_embedding_matrix(src_model, src_words)\n",
    "    Y = get_embedding_matrix(tgt_model, tgt_words)\n",
    "    \n",
    "    X_aligned = np.dot(X, W.T)\n",
    "    similarities = [cosine_similarity(X_aligned[i], Y[i]) for i in range(len(dictionary))]\n",
    "    \n",
    "    plt.figure(figsize=(8, 5))\n",
    "    plt.hist(similarities, bins=30, color='skyblue', edgecolor='black')\n",
    "    plt.xlabel(\"Cosine Similarity\")\n",
    "    plt.ylabel(\"Frequency\")\n",
    "    plt.title(\"Histogram of Cosine Similarities between Aligned English and Bengali Embeddings\")\n",
    "    hist_path = os.path.join(output_dir, \"cosine_similarity_histogram.png\")\n",
    "    plt.savefig(hist_path)\n",
    "    plt.close()\n",
    "    print(f\"Cosine similarity histogram saved to {hist_path}\")\n",
    "\n",
    "def visualize_singular_values(S, output_dir=\"visualizations\"):\n",
    "    \"\"\"\n",
    "    Plot the singular values from the SVD of the cross-covariance matrix.\n",
    "    \"\"\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    plt.figure(figsize=(8, 5))\n",
    "    plt.plot(S, marker='o', linestyle='-', color='orange')\n",
    "    plt.xlabel(\"Index\")\n",
    "    plt.ylabel(\"Singular Value\")\n",
    "    plt.title(\"Singular Values from SVD of Cross-Covariance Matrix\")\n",
    "    s_path = os.path.join(output_dir, \"singular_values.png\")\n",
    "    plt.savefig(s_path)\n",
    "    plt.close()\n",
    "    print(f\"Singular values plot saved to {s_path}\")\n",
    "\n",
    "def load_model_safe(model_path):\n",
    "    \"\"\"\n",
    "    Attempt to load a fastText model. If a MemoryError occurs, print a message and exit.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        model = fasttext.load_model(model_path)\n",
    "        return model\n",
    "    except MemoryError:\n",
    "        print(f\"MemoryError: Unable to load {model_path}.\")\n",
    "        print(\"Consider using a smaller fastText model or running on a machine with more RAM.\")\n",
    "        exit(1)\n",
    "\n",
    "def align_main():\n",
    "    # Set paths for the fastText models (ensure these files exist in your working directory)\n",
    "    src_model_path = 'cc.en.300.bin'\n",
    "    tgt_model_path = 'cc.bn.300.bin'\n",
    "    \n",
    "    print(\"Loading fastText models...\")\n",
    "    src_model = load_model_safe(src_model_path)\n",
    "    tgt_model = load_model_safe(tgt_model_path)\n",
    "    \n",
    "    # Path to your bilingual dictionary file (tab-separated)\n",
    "    dict_path = 'sutra_generated_ben_eng.txt'\n",
    "    dictionary = load_dictionary(dict_path)\n",
    "    \n",
    "    if not dictionary:\n",
    "        print(\"Error: The bilingual dictionary is empty or not properly formatted.\")\n",
    "        return\n",
    "    \n",
    "    print(f\"Loaded dictionary with {len(dictionary)} word pairs.\")\n",
    "    \n",
    "    print(\"Computing alignment matrix...\")\n",
    "    W, U, S, Vt = align_embeddings(src_model, tgt_model, dictionary)\n",
    "    \n",
    "    np.save(\"alignment_matrix_en_to_bn.npy\", W)\n",
    "    print(\"Alignment matrix saved to alignment_matrix_en_to_bn.npy\")\n",
    "    \n",
    "    visualize_alignment(src_model, tgt_model, dictionary, W)\n",
    "    visualize_singular_values(S)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading fastText model from cc.en.300.bin ...\n",
      "Model loaded successfully!\n",
      "Model saved to english_fasttext_model.bin\n",
      "cc.bn.300.bin.gz already exists, skipping download.\n",
      "cc.bn.300.bin already exists, skipping extraction.\n",
      "Loading fastText model from cc.bn.300.bin ...\n",
      "Model loaded successfully!\n",
      "Model saved to bengali_fasttext_model.bin\n"
     ]
    }
   ],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading fastText models...\n",
      "Loaded dictionary with 520 word pairs.\n",
      "Computing alignment matrix...\n",
      "Alignment matrix saved to alignment_matrix_en_to_bn.npy\n",
      "Cosine similarity histogram saved to visualizations\\cosine_similarity_histogram.png\n",
      "Singular values plot saved to visualizations\\singular_values.png\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    align_main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "English model path: c:\\Users\\TAMANG\\Documents\\GitHub\\research-cross-lingual-translation-english-bengali\\cc.en.300.bin\n",
      "Bengali model path: c:\\Users\\TAMANG\\Documents\\GitHub\\research-cross-lingual-translation-english-bengali\\cc.bn.300.bin\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "src_model_path = os.path.abspath('cc.en.300.bin')\n",
    "tgt_model_path = os.path.abspath('cc.bn.300.bin')\n",
    "print(\"English model path:\", src_model_path)\n",
    "print(\"Bengali model path:\", tgt_model_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading fastText model from cc.en.300.bin ...\n",
      "Model loaded successfully!\n",
      "Model saved to english_fasttext_model.bin\n",
      "cc.bn.300.bin.gz already exists, skipping download.\n",
      "Extracting cc.bn.300.bin.gz to cc.bn.300.bin ...\n",
      "Loading fastText model from cc.bn.300.bin ...\n",
      "Model loaded successfully!\n",
      "Model saved to bengali_fasttext_model.bin\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "import os\n",
    "import urllib.request\n",
    "import gzip\n",
    "import shutil\n",
    "import fasttext\n",
    "\n",
    "def download_and_extract(url, output_path):\n",
    "    \"\"\"\n",
    "    Download a .gz file from a URL and extract it to the specified output_path.\n",
    "    If the .gz or the extracted file already exists, skip that step.\n",
    "    \"\"\"\n",
    "    gz_path = output_path + \".gz\"\n",
    "    \n",
    "    # Download the gz file if it doesn't exist\n",
    "    if not os.path.exists(gz_path):\n",
    "        print(f\"Downloading {url} to {gz_path} ...\")\n",
    "        urllib.request.urlretrieve(url, gz_path)\n",
    "    else:\n",
    "        print(f\"{gz_path} already exists, skipping download.\")\n",
    "    \n",
    "    # Extract the file if the output_path doesn't exist\n",
    "    if not os.path.exists(output_path):\n",
    "        print(f\"Extracting {gz_path} to {output_path} ...\")\n",
    "        with gzip.open(gz_path, 'rb') as f_in, open(output_path, 'wb') as f_out:\n",
    "            shutil.copyfileobj(f_in, f_out)\n",
    "    else:\n",
    "        print(f\"{output_path} already exists, skipping extraction.\")\n",
    "\n",
    "def save_fasttext_model(model_path, new_model_path):\n",
    "    \"\"\"\n",
    "    Load a fastText model from model_path and save it to new_model_path.\n",
    "    \"\"\"\n",
    "    print(f\"Loading fastText model from {model_path} ...\")\n",
    "    model = fasttext.load_model(model_path)\n",
    "    print(\"Model loaded successfully!\")\n",
    "    \n",
    "    # Save the model to a new file\n",
    "    model.save_model(new_model_path)\n",
    "    print(f\"Model saved to {new_model_path}\")\n",
    "\n",
    "def main():\n",
    "    # # URL for the English fastText model (300-dimensional)\n",
    "    # en_url = \"https://dl.fbaipublicfiles.com/fasttext/vectors-crawl/cc.en.300.bin.gz\"\n",
    "    en_model_path = \"cc.en.300.bin\"  # The file after extraction\n",
    "    new_en_model_path = \"english_fasttext_model.bin\"  # Your desired saved file name\n",
    "\n",
    "    # # Download and extract the English model if needed\n",
    "    # download_and_extract(en_url, en_model_path)\n",
    "    \n",
    "    # # Load and save the English model\n",
    "    save_fasttext_model(en_model_path, new_en_model_path)\n",
    "    \n",
    "    # If you want to do the same for Bengali, uncomment and adjust the following:\n",
    "    bn_url = \"https://dl.fbaipublicfiles.com/fasttext/vectors-crawl/cc.bn.300.bin.gz\"\n",
    "    bn_model_path = \"cc.bn.300.bin\"\n",
    "    new_bn_model_path = \"bengali_fasttext_model.bin\"\n",
    "    \n",
    "    download_and_extract(bn_url, bn_model_path)\n",
    "    save_fasttext_model(bn_model_path, new_bn_model_path)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# nmt_model.py\n",
    "Model Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import math\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_len=5000):\n",
    "        super().__init__()\n",
    "        # Create a matrix of shape (max_len, d_model)\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        # Compute the positional encodings once in log space\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        # Change shape to (max_len, 1, d_model) so it can broadcast correctly over the batch dimension.\n",
    "        pe = pe.unsqueeze(1)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: Tensor of shape (seq_len, batch_size, d_model)\n",
    "        Returns:\n",
    "            x: Tensor with positional encoding added, shape (seq_len, batch_size, d_model)\n",
    "        \"\"\"\n",
    "        x = x + self.pe[:x.size(0)]\n",
    "        return x\n",
    "\n",
    "class MultilingualTransformer(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model=512, nhead=8, num_encoder_layers=6, \n",
    "                 num_decoder_layers=6, dim_feedforward=2048, dropout=0.1, max_seq_length=5000):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
    "        self.pos_encoder = PositionalEncoding(d_model, max_seq_length)\n",
    "        # Using PyTorch's Transformer module (it expects inputs in shape (seq_len, batch, d_model))\n",
    "        self.transformer = nn.Transformer(d_model, nhead, num_encoder_layers, num_decoder_layers, dim_feedforward, dropout)\n",
    "        self.fc_out = nn.Linear(d_model, vocab_size)\n",
    "        \n",
    "        # Parameter for language tag bias (can be used to steer attention toward target language tokens)\n",
    "        self.lang_tag_bias = nn.Parameter(torch.zeros(d_model))\n",
    "\n",
    "    def forward(self, src, tgt, src_mask=None, tgt_mask=None, src_key_padding_mask=None, tgt_key_padding_mask=None):\n",
    "        # src and tgt: shape (seq_len, batch_size)\n",
    "        src_emb = self.embedding(src) * math.sqrt(self.d_model)  # (src_len, batch, d_model)\n",
    "        tgt_emb = self.embedding(tgt) * math.sqrt(self.d_model)  # (tgt_len, batch, d_model)\n",
    "        \n",
    "        src_emb = self.pos_encoder(src_emb)\n",
    "        tgt_emb = self.pos_encoder(tgt_emb)\n",
    "        \n",
    "        # Inject language tag bias into the embeddings (a simple way to illustrate the idea)\n",
    "        src_emb = src_emb + self.lang_tag_bias\n",
    "        tgt_emb = tgt_emb + self.lang_tag_bias\n",
    "\n",
    "        # Pass through the transformer\n",
    "        output = self.transformer(src_emb, tgt_emb, src_mask=src_mask, tgt_mask=tgt_mask, \n",
    "                                  src_key_padding_mask=src_key_padding_mask, \n",
    "                                  tgt_key_padding_mask=tgt_key_padding_mask)\n",
    "        output = self.fc_out(output)  # (tgt_len, batch, vocab_size)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# train.py\n",
    "Training Script with Logging and Visualization + Aligning Embeddings\n",
    "\n",
    "### What This Code Does\n",
    "1. Model Initialization & Device Setup:\n",
    "    - It creates the MultilingualTransformer model, moves it to the proper device, and logs the status.\n",
    "\n",
    "2. Aligned Embeddings Integration:\n",
    "    - After the model is instantiated, the code:\n",
    "    - Loads the pretrained fastText model.\n",
    "    - Uses the load_aligned_embeddings function to compute an embedding matrix for the SentencePiece vocabulary.\n",
    "    - Copies these embeddings into the model’s embedding layer and (optionally) freezes them.\n",
    "\n",
    "3. Training Setup:\n",
    "    - The code then sets up the dataset, dataloader, loss function, optimizer, and training loop. It logs loss values and saves the training loss plot and model state.\n",
    "\n",
    "4. Running the Script:\n",
    "    - Run this updated train.py script. It will integrate the aligned embeddings before starting the training process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "# from nmt_model import MultilingualTransformer  # Ensure nmt_model.py is in the same directory\n",
    "import matplotlib.pyplot as plt\n",
    "import logging\n",
    "import sentencepiece as spm\n",
    "import numpy as np\n",
    "from fasttext import load_model  # for loading the fastText model\n",
    "import os\n",
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "# --------------------- Aligned Embeddings Functions --------------------- #\n",
    "def load_aligned_embeddings(sp_model_file, fasttext_model, alignment_matrix_path, vocab_size, d_model):\n",
    "    \"\"\"\n",
    "    Map tokens from your SentencePiece model to fastText embeddings,\n",
    "    apply the alignment transformation, and return a weight matrix of shape (vocab_size, d_model).\n",
    "    \"\"\"\n",
    "    sp = spm.SentencePieceProcessor(model_file=sp_model_file)\n",
    "    W = np.load(alignment_matrix_path)  # Expected shape: (d_model, d_model)\n",
    "    embedding_matrix = np.zeros((vocab_size, d_model))\n",
    "    for i in range(vocab_size):\n",
    "        token = sp.id_to_piece(i)\n",
    "        vector = fasttext_model.get_word_vector(token)  # Expected shape: (300,)\n",
    "        aligned_vector = np.dot(W, vector)             # Resulting shape: (300,)\n",
    "        embedding_matrix[i] = aligned_vector\n",
    "    return torch.tensor(embedding_matrix, dtype=torch.float)\n",
    "\n",
    "# --------------------- Dictionary Loader --------------------- #\n",
    "def load_dictionary(dict_path):\n",
    "    \"\"\"\n",
    "    Load a bilingual dictionary from a file.\n",
    "    Each line should have an English word and its Bengali translation separated by a tab.\n",
    "    Multi-word translations are preserved.\n",
    "    Returns a list of (english, bengali) pairs.\n",
    "    \"\"\"\n",
    "    dictionary = []\n",
    "    with open(dict_path, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            parts = line.strip().split('\\t', 1)\n",
    "            if len(parts) != 2:\n",
    "                continue\n",
    "            en_word = parts[0].strip()\n",
    "            bn_word = parts[1].strip()\n",
    "            dictionary.append((en_word, bn_word))\n",
    "    return dictionary\n",
    "\n",
    "# --------------------- Dataset and Collation --------------------- #\n",
    "class TokenizedDataset(Dataset):\n",
    "    def __init__(self, src_file, tgt_file, sp_model):\n",
    "        with open(src_file, 'r', encoding='utf-8') as f:\n",
    "            self.src_lines = f.readlines()\n",
    "        with open(tgt_file, 'r', encoding='utf-8') as f:\n",
    "            self.tgt_lines = f.readlines()\n",
    "        assert len(self.src_lines) == len(self.tgt_lines), \"Source and target files must have the same number of lines.\"\n",
    "        self.sp_model = sp_model\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.src_lines)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        src_ids = self.sp_model.encode_as_ids(self.src_lines[idx].strip())\n",
    "        tgt_ids = self.sp_model.encode_as_ids(self.tgt_lines[idx].strip())\n",
    "        return torch.tensor(src_ids, dtype=torch.long), torch.tensor(tgt_ids, dtype=torch.long)\n",
    "\n",
    "def collate_fn(batch):\n",
    "    src_batch, tgt_batch = zip(*batch)\n",
    "    max_src = max(len(seq) for seq in src_batch)\n",
    "    max_tgt = max(len(seq) for seq in tgt_batch)\n",
    "    padded_src = [torch.cat([seq, torch.zeros(max_src - len(seq), dtype=torch.long)]) for seq in src_batch]\n",
    "    padded_tgt = [torch.cat([seq, torch.zeros(max_tgt - len(seq), dtype=torch.long)]) for seq in tgt_batch]\n",
    "    padded_src = torch.stack(padded_src).transpose(0, 1)\n",
    "    padded_tgt = torch.stack(padded_tgt).transpose(0, 1)\n",
    "    return padded_src, padded_tgt\n",
    "\n",
    "# --------------------- Inference Function --------------------- #\n",
    "def greedy_decode(model, src, sp, max_len=50, start_token_id=1, end_token_id=0):\n",
    "    \"\"\"\n",
    "    Greedy decoding for a single source sentence.\n",
    "    Assumes src is a tensor of shape (seq_len, 1).\n",
    "    Returns a list of token ids.\n",
    "    \"\"\"\n",
    "    device = next(model.parameters()).device\n",
    "    src = src.to(device)\n",
    "    # Encode source sentence using the model's encoder\n",
    "    memory = model.transformer.encoder(model.pos_encoder(model.embedding(src)))\n",
    "    # Initialize ys with shape (1, batch)\n",
    "    ys = torch.tensor([[start_token_id]], dtype=torch.long, device=device)\n",
    "    for _ in range(max_len - 1):\n",
    "        tgt_mask = model.transformer.generate_square_subsequent_mask(ys.size(0)).to(device)\n",
    "        out = model.transformer.decoder(model.pos_encoder(model.embedding(ys)), memory, tgt_mask=tgt_mask)\n",
    "        out = model.fc_out(out[-1, :])\n",
    "        # next_word is computed with shape (batch,), then unsqueeze to (1, batch)\n",
    "        next_word = torch.argmax(out, dim=-1).unsqueeze(0)\n",
    "        if next_word.item() == end_token_id:\n",
    "            break\n",
    "        # Concatenate without unsqueezing further (so next_word remains shape (1, batch))\n",
    "        ys = torch.cat([ys, next_word], dim=0)\n",
    "    return ys.squeeze().tolist()\n",
    "\n",
    "def save_sample_translations(model, sp, sample_src_file, output_file, num_samples=10):\n",
    "    \"\"\"\n",
    "    Decode num_samples sentences from sample_src_file using greedy decoding\n",
    "    and save the source and generated translations for qualitative analysis.\n",
    "    \"\"\"\n",
    "    device = next(model.parameters()).device\n",
    "    with open(sample_src_file, 'r', encoding='utf-8') as f:\n",
    "        lines = f.readlines()\n",
    "    samples = lines[:num_samples]\n",
    "    results = []\n",
    "    for line in samples:\n",
    "        src_ids = sp.encode_as_ids(line.strip())\n",
    "        src_tensor = torch.tensor(src_ids, dtype=torch.long).unsqueeze(1)  # (seq_len, 1)\n",
    "        translation_ids = greedy_decode(model, src_tensor, sp)\n",
    "        translation = sp.decode_ids(translation_ids if isinstance(translation_ids, list) else [translation_ids])\n",
    "        results.append((line.strip(), translation))\n",
    "    \n",
    "    with open(output_file, 'w', encoding='utf-8') as f:\n",
    "        for src, trans in results:\n",
    "            f.write(\"Source: \" + src + \"\\n\")\n",
    "            f.write(\"Translation: \" + trans + \"\\n\\n\")\n",
    "    print(f\"Sample translations saved to {output_file}\")\n",
    "\n",
    "# --------------------- Training Function --------------------- #\n",
    "def train_model():\n",
    "    # Hyperparameters\n",
    "    vocab_size = 32000  # Must match your SentencePiece model\n",
    "    d_model = 300       # Set to 300 to match fastText dimensions\n",
    "    nhead = 6           # 300 is divisible by 6 (300/6 = 50)\n",
    "    num_encoder_layers = 3\n",
    "    num_decoder_layers = 3\n",
    "    num_epochs = 10\n",
    "    batch_size = 32\n",
    "    learning_rate = 1e-4\n",
    "    \n",
    "    # Initialize model (ensure MultilingualTransformer is defined in nmt_model.py)\n",
    "    model = MultilingualTransformer(vocab_size, d_model, nhead, num_encoder_layers, num_decoder_layers)\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model.to(device)\n",
    "    logging.info(\"Model initialized and moved to device: %s\", device)\n",
    "    \n",
    "    # ===== Integrate Aligned Embeddings =====\n",
    "    en_ft_model = load_model(\"english_fasttext_model.bin\")\n",
    "    aligned_embeds = load_aligned_embeddings(\"spm_model.model\", en_ft_model, \"alignment_matrix_en_to_bn.npy\", vocab_size, d_model)\n",
    "    model.embedding.weight.data.copy_(aligned_embeds)\n",
    "    model.embedding.weight.requires_grad = False  # Freeze embeddings\n",
    "    logging.info(\"Aligned embeddings integrated into the model.\")\n",
    "    # ===========================================\n",
    "    \n",
    "    criterion = nn.CrossEntropyLoss(ignore_index=0)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    \n",
    "    sp = spm.SentencePieceProcessor(model_file='spm_model.model')\n",
    "    train_dataset = TokenizedDataset('eng_tokenized.txt', 'ben_tokenized.txt', sp)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)\n",
    "    logging.info(\"Training data loaded. Total batches: %d\", len(train_loader))\n",
    "    \n",
    "    training_losses = []\n",
    "    \n",
    "    for epoch in range(1, num_epochs + 1):\n",
    "        model.train()\n",
    "        epoch_loss = 0.0\n",
    "        for batch_idx, (src, tgt) in enumerate(train_loader):\n",
    "            src, tgt = src.to(device), tgt.to(device)\n",
    "            tgt_input = tgt[:-1, :]   # Decoder input\n",
    "            tgt_output = tgt[1:, :]    # Expected output\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            tgt_mask = model.transformer.generate_square_subsequent_mask(tgt_input.size(0)).to(device)\n",
    "            output = model(src, tgt_input, tgt_mask=tgt_mask)\n",
    "            output = output.reshape(-1, vocab_size)\n",
    "            tgt_output = tgt_output.reshape(-1)\n",
    "            \n",
    "            loss = criterion(output, tgt_output)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            epoch_loss += loss.item()\n",
    "            if batch_idx % 10 == 0:\n",
    "                logging.info(\"Epoch %d, Batch %d, Loss: %.4f\", epoch, batch_idx, loss.item())\n",
    "        \n",
    "        avg_loss = epoch_loss / len(train_loader)\n",
    "        training_losses.append(avg_loss)\n",
    "        logging.info(\"Epoch %d complete, Average Loss: %.4f\", epoch, avg_loss)\n",
    "    \n",
    "    torch.save(model.state_dict(), \"multilingual_nmt_model.pt\")\n",
    "    logging.info(\"Model saved to multilingual_nmt_model.pt\")\n",
    "    \n",
    "    plt.figure(figsize=(8, 5))\n",
    "    plt.plot(range(1, num_epochs + 1), training_losses, marker='o')\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Average Loss\")\n",
    "    plt.title(\"Training Loss Over Epochs\")\n",
    "    plt.savefig(\"training_loss.png\")\n",
    "    plt.close()\n",
    "    logging.info(\"Training loss plot saved to training_loss.png\")\n",
    "    \n",
    "    # Save sample translations for qualitative evaluation\n",
    "    save_sample_translations(model, sp, 'eng_tokenized.txt', \"sample_translations.txt\", num_samples=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\TAMANG\\Documents\\GitHub\\research-cross-lingual-translation-english-bengali\\venv\\Lib\\site-packages\\torch\\nn\\modules\\transformer.py:385: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(\n",
      "2025-02-16 23:40:05,366 - INFO - Model initialized and moved to device: cpu\n",
      "2025-02-16 23:40:31,490 - INFO - Aligned embeddings integrated into the model.\n",
      "2025-02-16 23:40:31,602 - INFO - Training data loaded. Total batches: 32\n",
      "2025-02-16 23:40:48,473 - INFO - Epoch 1, Batch 0, Loss: 10.4155\n",
      "2025-02-16 23:41:06,517 - INFO - Epoch 1, Batch 10, Loss: 9.2117\n",
      "2025-02-16 23:41:18,064 - INFO - Epoch 1, Batch 20, Loss: 8.7462\n",
      "2025-02-16 23:41:29,027 - INFO - Epoch 1, Batch 30, Loss: 8.3767\n",
      "2025-02-16 23:41:29,465 - INFO - Epoch 1 complete, Average Loss: 8.9698\n",
      "2025-02-16 23:41:30,536 - INFO - Epoch 2, Batch 0, Loss: 8.0627\n",
      "2025-02-16 23:41:40,316 - INFO - Epoch 2, Batch 10, Loss: 7.7759\n",
      "2025-02-16 23:41:50,543 - INFO - Epoch 2, Batch 20, Loss: 7.4557\n",
      "2025-02-16 23:42:00,870 - INFO - Epoch 2, Batch 30, Loss: 7.4359\n",
      "2025-02-16 23:42:01,044 - INFO - Epoch 2 complete, Average Loss: 7.6949\n",
      "2025-02-16 23:42:02,001 - INFO - Epoch 3, Batch 0, Loss: 7.3280\n",
      "2025-02-16 23:42:11,351 - INFO - Epoch 3, Batch 10, Loss: 7.0365\n",
      "2025-02-16 23:42:21,098 - INFO - Epoch 3, Batch 20, Loss: 7.1265\n",
      "2025-02-16 23:42:31,484 - INFO - Epoch 3, Batch 30, Loss: 6.8586\n",
      "2025-02-16 23:42:31,690 - INFO - Epoch 3 complete, Average Loss: 7.0874\n",
      "2025-02-16 23:42:32,890 - INFO - Epoch 4, Batch 0, Loss: 6.9214\n",
      "2025-02-16 23:42:43,350 - INFO - Epoch 4, Batch 10, Loss: 6.7889\n",
      "2025-02-16 23:42:52,574 - INFO - Epoch 4, Batch 20, Loss: 6.7176\n",
      "2025-02-16 23:43:02,572 - INFO - Epoch 4, Batch 30, Loss: 6.7131\n",
      "2025-02-16 23:43:02,813 - INFO - Epoch 4 complete, Average Loss: 6.7230\n",
      "2025-02-16 23:43:03,759 - INFO - Epoch 5, Batch 0, Loss: 6.7077\n",
      "2025-02-16 23:43:13,795 - INFO - Epoch 5, Batch 10, Loss: 6.6590\n",
      "2025-02-16 23:43:23,168 - INFO - Epoch 5, Batch 20, Loss: 6.5553\n",
      "2025-02-16 23:43:32,316 - INFO - Epoch 5, Batch 30, Loss: 6.5218\n",
      "2025-02-16 23:43:32,538 - INFO - Epoch 5 complete, Average Loss: 6.5273\n",
      "2025-02-16 23:43:33,444 - INFO - Epoch 6, Batch 0, Loss: 6.6148\n",
      "2025-02-16 23:43:43,108 - INFO - Epoch 6, Batch 10, Loss: 6.5214\n",
      "2025-02-16 23:43:52,439 - INFO - Epoch 6, Batch 20, Loss: 6.4419\n",
      "2025-02-16 23:44:02,170 - INFO - Epoch 6, Batch 30, Loss: 6.4460\n",
      "2025-02-16 23:44:02,379 - INFO - Epoch 6 complete, Average Loss: 6.4257\n",
      "2025-02-16 23:44:03,362 - INFO - Epoch 7, Batch 0, Loss: 6.2710\n",
      "2025-02-16 23:44:13,176 - INFO - Epoch 7, Batch 10, Loss: 6.3453\n",
      "2025-02-16 23:44:22,963 - INFO - Epoch 7, Batch 20, Loss: 6.4277\n",
      "2025-02-16 23:44:32,866 - INFO - Epoch 7, Batch 30, Loss: 6.3911\n",
      "2025-02-16 23:44:33,108 - INFO - Epoch 7 complete, Average Loss: 6.3583\n",
      "2025-02-16 23:44:34,255 - INFO - Epoch 8, Batch 0, Loss: 6.2954\n",
      "2025-02-16 23:44:43,753 - INFO - Epoch 8, Batch 10, Loss: 6.3665\n",
      "2025-02-16 23:44:53,374 - INFO - Epoch 8, Batch 20, Loss: 6.4539\n",
      "2025-02-16 23:45:02,604 - INFO - Epoch 8, Batch 30, Loss: 6.3276\n",
      "2025-02-16 23:45:02,838 - INFO - Epoch 8 complete, Average Loss: 6.2975\n",
      "2025-02-16 23:45:03,675 - INFO - Epoch 9, Batch 0, Loss: 6.2826\n",
      "2025-02-16 23:45:13,756 - INFO - Epoch 9, Batch 10, Loss: 6.2906\n",
      "2025-02-16 23:45:23,583 - INFO - Epoch 9, Batch 20, Loss: 6.2003\n",
      "2025-02-16 23:45:33,604 - INFO - Epoch 9, Batch 30, Loss: 6.3228\n",
      "2025-02-16 23:45:33,806 - INFO - Epoch 9 complete, Average Loss: 6.2589\n",
      "2025-02-16 23:45:34,812 - INFO - Epoch 10, Batch 0, Loss: 6.2087\n",
      "2025-02-16 23:45:44,891 - INFO - Epoch 10, Batch 10, Loss: 6.1784\n",
      "2025-02-16 23:45:54,773 - INFO - Epoch 10, Batch 20, Loss: 6.1870\n",
      "2025-02-16 23:46:04,479 - INFO - Epoch 10, Batch 30, Loss: 6.0867\n",
      "2025-02-16 23:46:04,694 - INFO - Epoch 10 complete, Average Loss: 6.2040\n",
      "2025-02-16 23:46:06,818 - INFO - Model saved to multilingual_nmt_model.pt\n",
      "2025-02-16 23:46:07,112 - INFO - Training loss plot saved to training_loss.png\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample translations saved to sample_translations.txt\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    train_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading fastText models...\n",
      "Loaded dictionary with 520 word pairs.\n",
      "Computing alignment matrix...\n",
      "Alignment matrix saved to alignment_matrix_en_to_bn.npy\n",
      "Cosine similarity histogram saved to visualizations\\cosine_similarity_histogram.png\n",
      "Singular values plot saved to visualizations\\singular_values.png\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    align_main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# test_making.py\n",
    "\n",
    "Creating a sample test file (in order to evaluate) from flores dataset for English Bengali."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created test set with 1000 lines in 'eng_test.txt' and 'ben_test.txt'.\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "def create_test_set(src_file, tgt_file, test_src_file, test_tgt_file, num_lines=1000):\n",
    "    \"\"\"\n",
    "    Extracts the first num_lines from the source and target files and saves them as test sets.\n",
    "    \n",
    "    Parameters:\n",
    "      src_file (str): Path to the English FLORES file.\n",
    "      tgt_file (str): Path to the Bengali FLORES file.\n",
    "      test_src_file (str): Output file for the test English sentences.\n",
    "      test_tgt_file (str): Output file for the test Bengali sentences.\n",
    "      num_lines (int): Number of sentence pairs to extract.\n",
    "    \"\"\"\n",
    "    with open(src_file, 'r', encoding='utf-8') as f_src, open(tgt_file, 'r', encoding='utf-8') as f_tgt:\n",
    "        src_lines = f_src.readlines()\n",
    "        tgt_lines = f_tgt.readlines()\n",
    "    \n",
    "    # Check that the files have the same number of lines.\n",
    "    if len(src_lines) != len(tgt_lines):\n",
    "        raise ValueError(\"Source and target files do not have the same number of lines.\")\n",
    "    \n",
    "    # Select the first num_lines as test set.\n",
    "    test_src = src_lines[:num_lines]\n",
    "    test_tgt = tgt_lines[:num_lines]\n",
    "    \n",
    "    with open(test_src_file, 'w', encoding='utf-8') as f_test_src:\n",
    "        f_test_src.writelines(test_src)\n",
    "    with open(test_tgt_file, 'w', encoding='utf-8') as f_test_tgt:\n",
    "        f_test_tgt.writelines(test_tgt)\n",
    "    \n",
    "    print(f\"Created test set with {num_lines} lines in '{test_src_file}' and '{test_tgt_file}'.\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # Update these filenames as needed based on your FLORES dataset files.\n",
    "    create_test_set('eng.dev', 'ben.dev', 'eng_test.txt', 'ben_test.txt', num_lines=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# eval_nmt.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "import torch\n",
    "import sentencepiece as spm\n",
    "import sacrebleu\n",
    "import logging\n",
    "from fasttext import load_model\n",
    "# from nmt_model import MultilingualTransformer  # Ensure this file is in the same directory\n",
    "import numpy as np\n",
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "# Greedy decoding function (as defined earlier)\n",
    "def greedy_decode(model, src, sp, max_len=50, start_token_id=1, end_token_id=0):\n",
    "    \"\"\"\n",
    "    Greedy decoding for a single source sentence.\n",
    "    Assumes src is a tensor of shape (seq_len, 1).\n",
    "    Returns a list of token ids.\n",
    "    \"\"\"\n",
    "    device = next(model.parameters()).device\n",
    "    src = src.to(device)\n",
    "    memory = model.transformer.encoder(model.pos_encoder(model.embedding(src)))\n",
    "    ys = torch.tensor([[start_token_id]], dtype=torch.long, device=device)\n",
    "    for _ in range(max_len - 1):\n",
    "        tgt_mask = model.transformer.generate_square_subsequent_mask(ys.size(0)).to(device)\n",
    "        out = model.transformer.decoder(model.pos_encoder(model.embedding(ys)), memory, tgt_mask=tgt_mask)\n",
    "        out = model.fc_out(out[-1, :])\n",
    "        next_word = torch.argmax(out, dim=-1).unsqueeze(0)\n",
    "        if next_word.item() == end_token_id:\n",
    "            break\n",
    "        ys = torch.cat([ys, next_word], dim=0)\n",
    "    return ys.squeeze().tolist()\n",
    "\n",
    "def load_test_data(src_path, ref_path):\n",
    "    \"\"\"\n",
    "    Load test data from the provided paths.\n",
    "    Returns a list of source sentences and a list of reference translations.\n",
    "    \"\"\"\n",
    "    with open(src_path, 'r', encoding='utf-8') as f:\n",
    "        src_sentences = [line.strip() for line in f if line.strip()]\n",
    "    with open(ref_path, 'r', encoding='utf-8') as f:\n",
    "        # Assumes one reference per line; sacreBLEU accepts list of references (one or more)\n",
    "        ref_sentences = [line.strip() for line in f if line.strip()]\n",
    "    return src_sentences, ref_sentences\n",
    "\n",
    "def generate_translations(model, sp, src_sentences):\n",
    "    \"\"\"\n",
    "    Generate translations for a list of source sentences using greedy decoding.\n",
    "    Returns a list of generated translations (decoded as strings).\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    translations = []\n",
    "    for sentence in src_sentences:\n",
    "        src_ids = sp.encode_as_ids(sentence)\n",
    "        src_tensor = torch.tensor(src_ids, dtype=torch.long).unsqueeze(1)  # (seq_len, 1)\n",
    "        translation_ids = greedy_decode(model, src_tensor, sp)\n",
    "        translation = sp.decode_ids(translation_ids if isinstance(translation_ids, list) else [translation_ids])\n",
    "        translations.append(translation)\n",
    "    return translations\n",
    "\n",
    "def evaluate_model():\n",
    "    # Paths to files and models\n",
    "    test_src_path = \"eng_test.txt\"  # Test set English sentences\n",
    "    test_ref_path = \"ben_test.txt\"  # Corresponding reference Bengali translations\n",
    "    sp_model_path = \"spm_model.model\"\n",
    "    model_path = \"multilingual_nmt_model.pt\"\n",
    "    \n",
    "    # Hyperparameters should match those used during training\n",
    "    vocab_size = 32000\n",
    "    d_model = 300\n",
    "    nhead = 6\n",
    "    num_encoder_layers = 3\n",
    "    num_decoder_layers = 3\n",
    "    \n",
    "    # Load the SentencePiece model\n",
    "    sp = spm.SentencePieceProcessor(model_file=sp_model_path)\n",
    "    \n",
    "    # Initialize and load the trained model\n",
    "    model = MultilingualTransformer(vocab_size, d_model, nhead, num_encoder_layers, num_decoder_layers)\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model.to(device)\n",
    "    model.load_state_dict(torch.load(model_path, map_location=device))\n",
    "    logging.info(\"Trained model loaded from %s\", model_path)\n",
    "    \n",
    "    # Load test data\n",
    "    src_sentences, ref_sentences = load_test_data(test_src_path, test_ref_path)\n",
    "    logging.info(\"Loaded %d test sentences.\", len(src_sentences))\n",
    "    \n",
    "    # Generate translations\n",
    "    generated_translations = generate_translations(model, sp, src_sentences)\n",
    "    \n",
    "    # Save generated translations for qualitative analysis\n",
    "    with open(\"generated_translations.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "        for src, trans in zip(src_sentences, generated_translations):\n",
    "            f.write(\"Source: \" + src + \"\\n\")\n",
    "            f.write(\"Translation: \" + trans + \"\\n\\n\")\n",
    "    logging.info(\"Generated translations saved to generated_translations.txt\")\n",
    "    \n",
    "    # Compute BLEU score using sacreBLEU\n",
    "    # sacreBLEU expects references as a list of lists\n",
    "    bleu = sacrebleu.corpus_bleu(generated_translations, [ref_sentences])\n",
    "    print(\"BLEU score:\", bleu.score)\n",
    "    logging.info(\"BLEU score: %.2f\", bleu.score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-17 11:03:22,118 - INFO - Trained model loaded from multilingual_nmt_model.pt\n",
      "2025-02-17 11:03:22,134 - INFO - Loaded 997 test sentences.\n",
      "2025-02-17 11:11:29,028 - INFO - Generated translations saved to generated_translations.txt\n",
      "2025-02-17 11:11:29,149 - INFO - BLEU score: 0.00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU score: 0.0\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    evaluate_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# eval_nmt_2.py\n",
    "\n",
    "Evaluation attempt 2, & this time:\n",
    "- Updated version of your inference code that uses beam search instead of greedy decoding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-17 11:33:02,161 - INFO - Trained model loaded from multilingual_nmt_model.pt\n",
      "2025-02-17 11:33:02,165 - INFO - Loaded 997 test sentences.\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "import torch\n",
    "import sentencepiece as spm\n",
    "import sacrebleu\n",
    "import logging\n",
    "from fasttext import load_model\n",
    "# from nmt_model import MultilingualTransformer  # Ensure this file is in the same directory\n",
    "import os\n",
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "def beam_search_decode(model, src, sp, beam_width=3, max_len=50, start_token_id=1, end_token_id=0):\n",
    "    \"\"\"\n",
    "    Beam search decoding for a single source sentence.\n",
    "    Assumes src is a tensor of shape (seq_len, 1).\n",
    "    Returns the best candidate sequence as a list of token ids.\n",
    "    \"\"\"\n",
    "    device = next(model.parameters()).device\n",
    "    src = src.to(device)\n",
    "    memory = model.transformer.encoder(model.pos_encoder(model.embedding(src)))\n",
    "    # Initialize the beam with the start token. Ensure it is a 1D tensor.\n",
    "    beam = [(torch.tensor([start_token_id], dtype=torch.long, device=device), 0.0)]\n",
    "    \n",
    "    for _ in range(max_len - 1):\n",
    "        new_beam = []\n",
    "        for seq, score in beam:\n",
    "            seq = seq.view(-1)  # Ensure seq is 1D\n",
    "            if seq[-1].item() == end_token_id:\n",
    "                new_beam.append((seq, score))\n",
    "                continue\n",
    "            seq_input = seq.unsqueeze(1)  # shape: (current_seq_len, 1)\n",
    "            tgt_mask = model.transformer.generate_square_subsequent_mask(seq_input.size(0)).to(device)\n",
    "            out = model.transformer.decoder(model.pos_encoder(model.embedding(seq_input)), memory, tgt_mask=tgt_mask)\n",
    "            logits = model.fc_out(out[-1, :])  # shape: (batch, vocab_size), batch==1 so shape is (1, vocab_size)\n",
    "            log_probs = torch.log_softmax(logits, dim=-1)  # shape: (1, vocab_size)\n",
    "            # Get top beam_width candidates. Now topk_* will have shape (1, beam_width)\n",
    "            topk_log_probs, topk_indices = torch.topk(log_probs, beam_width, dim=-1)\n",
    "            # Iterate over beam_width candidates in the first row.\n",
    "            for j in range(beam_width):\n",
    "                candidate = topk_indices[0, j].unsqueeze(0)  # shape: (1,)\n",
    "                candidate_log_prob = topk_log_probs[0, j].item()\n",
    "                new_seq = torch.cat([seq, candidate], dim=0)\n",
    "                new_score = score + candidate_log_prob\n",
    "                new_beam.append((new_seq, new_score))\n",
    "        # Keep only the top beam_width candidates\n",
    "        beam = sorted(new_beam, key=lambda x: x[1], reverse=True)[:beam_width]\n",
    "        if all(seq[-1].item() == end_token_id for seq, _ in beam):\n",
    "            break\n",
    "    best_seq, best_score = beam[0]\n",
    "    return best_seq.squeeze().tolist()\n",
    "\n",
    "def generate_translations_beam(model, sp, src_sentences, beam_width=3):\n",
    "    model.eval()\n",
    "    translations = []\n",
    "    for sentence in src_sentences:\n",
    "        src_ids = sp.encode_as_ids(sentence.strip())\n",
    "        src_tensor = torch.tensor(src_ids, dtype=torch.long).unsqueeze(1)\n",
    "        translation_ids = beam_search_decode(model, src_tensor, sp, beam_width=beam_width)\n",
    "        translation = sp.decode_ids(translation_ids if isinstance(translation_ids, list) else [translation_ids])\n",
    "        translations.append(translation)\n",
    "    return translations\n",
    "\n",
    "def load_test_data(src_path, ref_path):\n",
    "    with open(src_path, 'r', encoding='utf-8') as f:\n",
    "        src_sentences = [line.strip() for line in f if line.strip()]\n",
    "    with open(ref_path, 'r', encoding='utf-8') as f:\n",
    "        ref_sentences = [line.strip() for line in f if line.strip()]\n",
    "    return src_sentences, ref_sentences\n",
    "\n",
    "def evaluate_model():\n",
    "    test_src_path = \"eng_test.txt\"    # Test set English sentences\n",
    "    test_ref_path = \"ben_test.txt\"     # Corresponding Bengali references\n",
    "    sp_model_path = \"spm_model.model\"\n",
    "    model_path = \"multilingual_nmt_model.pt\"\n",
    "    \n",
    "    vocab_size = 32000\n",
    "    d_model = 300\n",
    "    nhead = 6\n",
    "    num_encoder_layers = 3\n",
    "    num_decoder_layers = 3\n",
    "    \n",
    "    sp = spm.SentencePieceProcessor(model_file=sp_model_path)\n",
    "    model = MultilingualTransformer(vocab_size, d_model, nhead, num_encoder_layers, num_decoder_layers)\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model.to(device)\n",
    "    model.load_state_dict(torch.load(model_path, map_location=device))\n",
    "    logging.info(\"Trained model loaded from %s\", model_path)\n",
    "    \n",
    "    src_sentences, ref_sentences = load_test_data(\"eng_test.txt\", \"ben_test.txt\")\n",
    "    logging.info(\"Loaded %d test sentences.\", len(src_sentences))\n",
    "    \n",
    "    generated_translations = generate_translations_beam(model, sp, src_sentences, beam_width=3)\n",
    "    \n",
    "    with open(\"generated_translations_beam.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "        for src, trans in zip(src_sentences, generated_translations):\n",
    "            f.write(\"Source: \" + src + \"\\n\")\n",
    "            f.write(\"Translation: \" + trans + \"\\n\\n\")\n",
    "    logging.info(\"Generated translations saved to generated_translations_beam.txt\")\n",
    "    \n",
    "    bleu = sacrebleu.corpus_bleu(generated_translations, [ref_sentences])\n",
    "    logging.info(\"BLEU score: %.2f\", bleu.score)\n",
    "    print(\"BLEU score:\", bleu.score)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    evaluate_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
