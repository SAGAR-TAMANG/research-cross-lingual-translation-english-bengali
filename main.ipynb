{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# main.py "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "import os\n",
    "import re\n",
    "import unicodedata\n",
    "import sentencepiece as spm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_unicode(text):\n",
    "    \"\"\"\n",
    "    Normalize text using NFC Unicode normalization.\n",
    "    \"\"\"\n",
    "    return unicodedata.normalize('NFC', text)\n",
    "\n",
    "def clean_text(text, lang='en'):\n",
    "    \"\"\"\n",
    "    Clean and normalize text:\n",
    "      - Strips extra spaces.\n",
    "      - Applies Unicode normalization.\n",
    "      - Removes extra punctuation characters (customize as needed).\n",
    "      - Converts English text to lowercase.\n",
    "    \"\"\"\n",
    "    text = text.strip()\n",
    "    text = normalize_unicode(text)\n",
    "    # Replace multiple spaces with a single space\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    # Remove some unwanted punctuation (customize if needed)\n",
    "    text = re.sub(r'[“”‘’]', '', text)\n",
    "    if lang == 'en':\n",
    "        text = text.lower()\n",
    "    return text\n",
    "\n",
    "def add_language_tag(text, lang_tag):\n",
    "    \"\"\"\n",
    "    Prepend a language tag to a sentence.\n",
    "    \"\"\"\n",
    "    return f\"{lang_tag} {text}\"\n",
    "\n",
    "def preprocess_file(input_file, output_file, lang_tag, lang='en'):\n",
    "    \"\"\"\n",
    "    Reads an input file line by line, cleans each line,\n",
    "    prepends a language tag, and writes the result to output_file.\n",
    "    \"\"\"\n",
    "    lines_cleaned = []\n",
    "    with open(input_file, 'r', encoding='utf-8') as f_in:\n",
    "        lines = f_in.readlines()\n",
    "        for line in lines:\n",
    "            cleaned = clean_text(line, lang=lang)\n",
    "            tagged = add_language_tag(cleaned, lang_tag)\n",
    "            lines_cleaned.append(tagged)\n",
    "    \n",
    "    with open(output_file, 'w', encoding='utf-8') as f_out:\n",
    "        for line in lines_cleaned:\n",
    "            f_out.write(line + \"\\n\")\n",
    "    \n",
    "    print(f\"Preprocessed {input_file} -> {output_file}\")\n",
    "    return output_file\n",
    "\n",
    "def combine_files(file_list, combined_file):\n",
    "    \"\"\"\n",
    "    Combine multiple text files into one.\n",
    "    \"\"\"\n",
    "    with open(combined_file, 'w', encoding='utf-8') as f_out:\n",
    "        for file in file_list:\n",
    "            with open(file, 'r', encoding='utf-8') as f_in:\n",
    "                f_out.write(f_in.read())\n",
    "    print(f\"Combined files into {combined_file}\")\n",
    "    return combined_file\n",
    "\n",
    "def train_sentencepiece_model(input_file, model_prefix, vocab_size=32000):\n",
    "    \"\"\"\n",
    "    Train a SentencePiece model on the provided input file.\n",
    "    This generates two files: {model_prefix}.model and {model_prefix}.vocab.\n",
    "    \"\"\"\n",
    "    spm.SentencePieceTrainer.train(\n",
    "        f\"--input={input_file} --model_prefix={model_prefix} --vocab_size={vocab_size} \"\n",
    "        f\"--model_type=bpe --character_coverage=1.0\"\n",
    "    )\n",
    "    print(f\"Trained SentencePiece model: {model_prefix}.model and {model_prefix}.vocab\")\n",
    "\n",
    "def tokenize_file(input_file, output_file, model_file):\n",
    "    \"\"\"\n",
    "    Tokenize an input file using the trained SentencePiece model,\n",
    "    and write the tokenized sentences to the output file.\n",
    "    \"\"\"\n",
    "    sp = spm.SentencePieceProcessor(model_file=model_file)\n",
    "    with open(input_file, 'r', encoding='utf-8') as f_in, \\\n",
    "         open(output_file, 'w', encoding='utf-8') as f_out:\n",
    "        for line in f_in:\n",
    "            pieces = sp.encode_as_pieces(line.strip())\n",
    "            f_out.write(\" \".join(pieces) + \"\\n\")\n",
    "    print(f\"Tokenized {input_file} -> {output_file}\")\n",
    "\n",
    "def main():\n",
    "    # Define file names for your development data\n",
    "    eng_dev = 'eng.dev'\n",
    "    ben_dev = 'ben.dev'\n",
    "    \n",
    "    # Output file names for the cleaned data\n",
    "    eng_clean = 'eng_clean.txt'\n",
    "    ben_clean = 'ben_clean.txt'\n",
    "    \n",
    "    # Combine cleaned file for SentencePiece training\n",
    "    combined_file = 'combined.txt'\n",
    "    \n",
    "    # SentencePiece model settings\n",
    "    model_prefix = 'spm_model'\n",
    "    model_file = f\"{model_prefix}.model\"\n",
    "    \n",
    "    # Output file names for tokenized data\n",
    "    eng_tokenized = 'eng_tokenized.txt'\n",
    "    ben_tokenized = 'ben_tokenized.txt'\n",
    "    \n",
    "    # 1. Preprocess each file (clean text and add language tags)\n",
    "    preprocess_file(eng_dev, eng_clean, '<en>', lang='en')\n",
    "    preprocess_file(ben_dev, ben_clean, '<bn>', lang='bn')\n",
    "    \n",
    "    # 2. Combine the cleaned files into one for training the tokenizer\n",
    "    combine_files([eng_clean, ben_clean], combined_file)\n",
    "    \n",
    "    # 3. Train the SentencePiece model on the combined data\n",
    "    train_sentencepiece_model(combined_file, model_prefix, vocab_size=32000)\n",
    "    \n",
    "    # 4. Tokenize the cleaned files using the trained SentencePiece model\n",
    "    tokenize_file(eng_clean, eng_tokenized, model_file)\n",
    "    tokenize_file(ben_clean, ben_tokenized, model_file)\n",
    "    \n",
    "    print(\"Preprocessing and tokenization complete.\")\n",
    "\n",
    "# if __name__ == '__main__':\n",
    "#     main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessed eng.dev -> eng_clean.txt\n",
      "Preprocessed ben.dev -> ben_clean.txt\n",
      "Combined files into combined.txt\n",
      "Trained SentencePiece model: spm_model.model and spm_model.vocab\n",
      "Tokenized eng_clean.txt -> eng_tokenized.txt\n",
      "Tokenized ben_clean.txt -> ben_tokenized.txt\n",
      "Preprocessing and tokenization complete.\n"
     ]
    }
   ],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# lexicon_generator.py\n",
    "\n",
    "- It reads parallel sentences from the two files.\n",
    "- It tokenizes the sentences by splitting on whitespace (you may later improve tokenization using a more sophisticated tokenizer).\n",
    "- It counts how often each English word appears and how often each English word co-occurs with each Bengali word in the same sentence pair.\n",
    "- For each English word that appears frequently enough (above a threshold), it selects the Bengali word with the highest conditional probability (i.e. highest relative co-occurrence frequency) as its candidate translation.\n",
    "- Finally, it writes the resulting lexicon (as English–Bengali pairs) to an output file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building co-occurrence counts from FLORES 101 data...\n",
      "Choosing candidate translations based on conditional probabilities...\n",
      "Generated lexicon with 520 entries.\n",
      "Lexicon saved to auto_lexicon.txt\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "import collections\n",
    "import math\n",
    "\n",
    "def tokenize(text):\n",
    "    \"\"\"\n",
    "    Simple whitespace tokenization.\n",
    "    You can later replace this with a more sophisticated tokenizer if needed.\n",
    "    \"\"\"\n",
    "    return text.strip().split()\n",
    "\n",
    "def build_cooccurrence(eng_file, ben_file):\n",
    "    \"\"\"\n",
    "    Reads parallel sentences from eng_file and ben_file,\n",
    "    and builds frequency counts and co-occurrence counts.\n",
    "    Returns:\n",
    "      eng_counts: Counter for English word frequencies.\n",
    "      cooc_counts: Dictionary mapping an English word to a Counter of Bengali words.\n",
    "    \"\"\"\n",
    "    eng_counts = collections.Counter()\n",
    "    cooc_counts = {}  # eng_word -> Counter({ben_word: count})\n",
    "    \n",
    "    with open(eng_file, 'r', encoding='utf-8') as ef, open(ben_file, 'r', encoding='utf-8') as bf:\n",
    "        eng_lines = ef.readlines()\n",
    "        ben_lines = bf.readlines()\n",
    "    \n",
    "    if len(eng_lines) != len(ben_lines):\n",
    "        raise ValueError(\"The number of lines in the English and Bengali files must be equal.\")\n",
    "    \n",
    "    for eng_line, ben_line in zip(eng_lines, ben_lines):\n",
    "        eng_tokens = tokenize(eng_line)\n",
    "        ben_tokens = tokenize(ben_line)\n",
    "        \n",
    "        # Update counts for each English token in this sentence\n",
    "        for e in eng_tokens:\n",
    "            eng_counts[e] += 1\n",
    "            if e not in cooc_counts:\n",
    "                cooc_counts[e] = collections.Counter()\n",
    "            # Count all Bengali tokens as co-occurring with this English token\n",
    "            for b in ben_tokens:\n",
    "                cooc_counts[e][b] += 1\n",
    "                \n",
    "    return eng_counts, cooc_counts\n",
    "\n",
    "def choose_translation(eng_counts, cooc_counts, min_count=5):\n",
    "    \"\"\"\n",
    "    For each English word with frequency >= min_count, choose the Bengali word\n",
    "    that maximizes the conditional probability P(b|e) = count(e,b)/count(e).\n",
    "    Returns a dictionary mapping English word to candidate Bengali translation.\n",
    "    \"\"\"\n",
    "    lexicon = {}\n",
    "    for e, freq in eng_counts.items():\n",
    "        if freq < min_count:\n",
    "            continue  # skip rare words\n",
    "        best_b = None\n",
    "        best_prob = 0.0\n",
    "        for b, cooc in cooc_counts[e].items():\n",
    "            prob = cooc / freq  # conditional probability P(b|e)\n",
    "            if prob > best_prob:\n",
    "                best_prob = prob\n",
    "                best_b = b\n",
    "        if best_b is not None:\n",
    "            lexicon[e] = best_b\n",
    "    return lexicon\n",
    "\n",
    "def save_lexicon(lexicon, output_file):\n",
    "    \"\"\"\n",
    "    Writes the lexicon to output_file, one pair per line, tab-separated.\n",
    "    \"\"\"\n",
    "    with open(output_file, 'w', encoding='utf-8') as f:\n",
    "        for e, b in lexicon.items():\n",
    "            f.write(f\"{e}\\t{b}\\n\")\n",
    "    print(f\"Lexicon saved to {output_file}\")\n",
    "\n",
    "def main():\n",
    "    eng_file = \"eng.dev\"\n",
    "    ben_file = \"ben.dev\"\n",
    "    output_file = \"auto_lexicon.txt\"\n",
    "    min_count = 5  # You can adjust this threshold based on your data\n",
    "    \n",
    "    print(\"Building co-occurrence counts from FLORES 101 data...\")\n",
    "    eng_counts, cooc_counts = build_cooccurrence(eng_file, ben_file)\n",
    "    \n",
    "    print(\"Choosing candidate translations based on conditional probabilities...\")\n",
    "    lexicon = choose_translation(eng_counts, cooc_counts, min_count=min_count)\n",
    "    \n",
    "    print(f\"Generated lexicon with {len(lexicon)} entries.\")\n",
    "    save_lexicon(lexicon, output_file)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The generated lexicon sucks, we would need to go manual, or api method."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# align_embeddings.py\n",
    "Alignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "import fasttext\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from numpy.linalg import norm, svd\n",
    "\n",
    "def load_dictionary(dict_path):\n",
    "    \"\"\"\n",
    "    Load a bilingual dictionary from a file.\n",
    "    Each line should contain an English word and its Bengali translation separated by a tab.\n",
    "    Multi-word translations are preserved.\n",
    "    Returns a list of (english, bengali) pairs.\n",
    "    \"\"\"\n",
    "    dictionary = []\n",
    "    with open(dict_path, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            parts = line.strip().split('\\t', 1)\n",
    "            if len(parts) != 2:\n",
    "                continue  # Skip malformed lines\n",
    "            en_word = parts[0].strip()\n",
    "            bn_word = parts[1].strip()\n",
    "            dictionary.append((en_word, bn_word))\n",
    "    return dictionary\n",
    "\n",
    "def get_embedding_matrix(model, words):\n",
    "    \"\"\"\n",
    "    For a list of words, return a matrix (numpy array) where each row is the embedding of that word.\n",
    "    \"\"\"\n",
    "    embeddings = []\n",
    "    for word in words:\n",
    "        vector = model.get_word_vector(word)\n",
    "        embeddings.append(vector)\n",
    "    return np.array(embeddings)\n",
    "\n",
    "def align_embeddings(src_model, tgt_model, dictionary):\n",
    "    \"\"\"\n",
    "    Align embeddings from the source (English) to target (Bengali) space using Procrustes analysis.\n",
    "    Returns the transformation matrix and SVD components for visualization.\n",
    "    \"\"\"\n",
    "    src_words = [pair[0] for pair in dictionary]\n",
    "    tgt_words = [pair[1] for pair in dictionary]\n",
    "    \n",
    "    X = get_embedding_matrix(src_model, src_words)  # English embeddings\n",
    "    Y = get_embedding_matrix(tgt_model, tgt_words)  # Bengali embeddings\n",
    "    \n",
    "    M = np.dot(Y.T, X)  # Cross-covariance matrix\n",
    "    U, S, Vt = svd(M)\n",
    "    W = np.dot(U, Vt)\n",
    "    return W, U, S, Vt\n",
    "\n",
    "def cosine_similarity(a, b):\n",
    "    return np.dot(a, b) / (norm(a) * norm(b) + 1e-8)\n",
    "\n",
    "def visualize_alignment(src_model, tgt_model, dictionary, W, output_dir=\"visualizations\"):\n",
    "    \"\"\"\n",
    "    Visualize the alignment by plotting a histogram of cosine similarities between \n",
    "    transformed English embeddings and Bengali embeddings.\n",
    "    \"\"\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    src_words = [pair[0] for pair in dictionary]\n",
    "    tgt_words = [pair[1] for pair in dictionary]\n",
    "    X = get_embedding_matrix(src_model, src_words)\n",
    "    Y = get_embedding_matrix(tgt_model, tgt_words)\n",
    "    \n",
    "    X_aligned = np.dot(X, W.T)\n",
    "    similarities = [cosine_similarity(X_aligned[i], Y[i]) for i in range(len(dictionary))]\n",
    "    \n",
    "    plt.figure(figsize=(8, 5))\n",
    "    plt.hist(similarities, bins=30, color='skyblue', edgecolor='black')\n",
    "    plt.xlabel(\"Cosine Similarity\")\n",
    "    plt.ylabel(\"Frequency\")\n",
    "    plt.title(\"Histogram of Cosine Similarities between Aligned English and Bengali Embeddings\")\n",
    "    hist_path = os.path.join(output_dir, \"cosine_similarity_histogram.png\")\n",
    "    plt.savefig(hist_path)\n",
    "    plt.close()\n",
    "    print(f\"Cosine similarity histogram saved to {hist_path}\")\n",
    "\n",
    "def visualize_singular_values(S, output_dir=\"visualizations\"):\n",
    "    \"\"\"\n",
    "    Plot the singular values from the SVD of the cross-covariance matrix.\n",
    "    \"\"\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    plt.figure(figsize=(8, 5))\n",
    "    plt.plot(S, marker='o', linestyle='-', color='orange')\n",
    "    plt.xlabel(\"Index\")\n",
    "    plt.ylabel(\"Singular Value\")\n",
    "    plt.title(\"Singular Values from SVD of Cross-Covariance Matrix\")\n",
    "    s_path = os.path.join(output_dir, \"singular_values.png\")\n",
    "    plt.savefig(s_path)\n",
    "    plt.close()\n",
    "    print(f\"Singular values plot saved to {s_path}\")\n",
    "\n",
    "def load_model_safe(model_path):\n",
    "    \"\"\"\n",
    "    Attempt to load a fastText model. If a MemoryError occurs, print a message and exit.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        model = fasttext.load_model(model_path)\n",
    "        return model\n",
    "    except MemoryError:\n",
    "        print(f\"MemoryError: Unable to load {model_path}.\")\n",
    "        print(\"Consider using a smaller fastText model or running on a machine with more RAM.\")\n",
    "        exit(1)\n",
    "\n",
    "def align_main():\n",
    "    # Set paths for the fastText models (ensure these files exist in your working directory)\n",
    "    src_model_path = 'cc.en.300.bin'\n",
    "    tgt_model_path = 'cc.bn.300.bin'\n",
    "    \n",
    "    print(\"Loading fastText models...\")\n",
    "    src_model = load_model_safe(src_model_path)\n",
    "    tgt_model = load_model_safe(tgt_model_path)\n",
    "    \n",
    "    # Path to your bilingual dictionary file (tab-separated)\n",
    "    dict_path = 'sutra_generated_ben_eng.txt'\n",
    "    dictionary = load_dictionary(dict_path)\n",
    "    \n",
    "    if not dictionary:\n",
    "        print(\"Error: The bilingual dictionary is empty or not properly formatted.\")\n",
    "        return\n",
    "    \n",
    "    print(f\"Loaded dictionary with {len(dictionary)} word pairs.\")\n",
    "    \n",
    "    print(\"Computing alignment matrix...\")\n",
    "    W, U, S, Vt = align_embeddings(src_model, tgt_model, dictionary)\n",
    "    \n",
    "    np.save(\"alignment_matrix_en_to_bn.npy\", W)\n",
    "    print(\"Alignment matrix saved to alignment_matrix_en_to_bn.npy\")\n",
    "    \n",
    "    visualize_alignment(src_model, tgt_model, dictionary, W)\n",
    "    visualize_singular_values(S)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading fastText model from cc.en.300.bin ...\n",
      "Model loaded successfully!\n",
      "Model saved to english_fasttext_model.bin\n",
      "cc.bn.300.bin.gz already exists, skipping download.\n",
      "cc.bn.300.bin already exists, skipping extraction.\n",
      "Loading fastText model from cc.bn.300.bin ...\n",
      "Model loaded successfully!\n",
      "Model saved to bengali_fasttext_model.bin\n"
     ]
    }
   ],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading fastText models...\n",
      "Loaded dictionary with 520 word pairs.\n",
      "Computing alignment matrix...\n",
      "Alignment matrix saved to alignment_matrix_en_to_bn.npy\n",
      "Cosine similarity histogram saved to visualizations\\cosine_similarity_histogram.png\n",
      "Singular values plot saved to visualizations\\singular_values.png\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    align_main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "English model path: c:\\Users\\TAMANG\\Documents\\GitHub\\research-cross-lingual-translation-english-bengali\\cc.en.300.bin\n",
      "Bengali model path: c:\\Users\\TAMANG\\Documents\\GitHub\\research-cross-lingual-translation-english-bengali\\cc.bn.300.bin\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "src_model_path = os.path.abspath('cc.en.300.bin')\n",
    "tgt_model_path = os.path.abspath('cc.bn.300.bin')\n",
    "print(\"English model path:\", src_model_path)\n",
    "print(\"Bengali model path:\", tgt_model_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading fastText model from cc.en.300.bin ...\n",
      "Model loaded successfully!\n",
      "Model saved to english_fasttext_model.bin\n",
      "cc.bn.300.bin.gz already exists, skipping download.\n",
      "Extracting cc.bn.300.bin.gz to cc.bn.300.bin ...\n",
      "Loading fastText model from cc.bn.300.bin ...\n",
      "Model loaded successfully!\n",
      "Model saved to bengali_fasttext_model.bin\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "import os\n",
    "import urllib.request\n",
    "import gzip\n",
    "import shutil\n",
    "import fasttext\n",
    "\n",
    "def download_and_extract(url, output_path):\n",
    "    \"\"\"\n",
    "    Download a .gz file from a URL and extract it to the specified output_path.\n",
    "    If the .gz or the extracted file already exists, skip that step.\n",
    "    \"\"\"\n",
    "    gz_path = output_path + \".gz\"\n",
    "    \n",
    "    # Download the gz file if it doesn't exist\n",
    "    if not os.path.exists(gz_path):\n",
    "        print(f\"Downloading {url} to {gz_path} ...\")\n",
    "        urllib.request.urlretrieve(url, gz_path)\n",
    "    else:\n",
    "        print(f\"{gz_path} already exists, skipping download.\")\n",
    "    \n",
    "    # Extract the file if the output_path doesn't exist\n",
    "    if not os.path.exists(output_path):\n",
    "        print(f\"Extracting {gz_path} to {output_path} ...\")\n",
    "        with gzip.open(gz_path, 'rb') as f_in, open(output_path, 'wb') as f_out:\n",
    "            shutil.copyfileobj(f_in, f_out)\n",
    "    else:\n",
    "        print(f\"{output_path} already exists, skipping extraction.\")\n",
    "\n",
    "def save_fasttext_model(model_path, new_model_path):\n",
    "    \"\"\"\n",
    "    Load a fastText model from model_path and save it to new_model_path.\n",
    "    \"\"\"\n",
    "    print(f\"Loading fastText model from {model_path} ...\")\n",
    "    model = fasttext.load_model(model_path)\n",
    "    print(\"Model loaded successfully!\")\n",
    "    \n",
    "    # Save the model to a new file\n",
    "    model.save_model(new_model_path)\n",
    "    print(f\"Model saved to {new_model_path}\")\n",
    "\n",
    "def main():\n",
    "    # # URL for the English fastText model (300-dimensional)\n",
    "    # en_url = \"https://dl.fbaipublicfiles.com/fasttext/vectors-crawl/cc.en.300.bin.gz\"\n",
    "    en_model_path = \"cc.en.300.bin\"  # The file after extraction\n",
    "    new_en_model_path = \"english_fasttext_model.bin\"  # Your desired saved file name\n",
    "\n",
    "    # # Download and extract the English model if needed\n",
    "    # download_and_extract(en_url, en_model_path)\n",
    "    \n",
    "    # # Load and save the English model\n",
    "    save_fasttext_model(en_model_path, new_en_model_path)\n",
    "    \n",
    "    # If you want to do the same for Bengali, uncomment and adjust the following:\n",
    "    bn_url = \"https://dl.fbaipublicfiles.com/fasttext/vectors-crawl/cc.bn.300.bin.gz\"\n",
    "    bn_model_path = \"cc.bn.300.bin\"\n",
    "    new_bn_model_path = \"bengali_fasttext_model.bin\"\n",
    "    \n",
    "    download_and_extract(bn_url, bn_model_path)\n",
    "    save_fasttext_model(bn_model_path, new_bn_model_path)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# nmt_model.py\n",
    "Model Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import math\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_len=5000):\n",
    "        super().__init__()\n",
    "        # Create a matrix of shape (max_len, d_model)\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        # Compute the positional encodings once in log space\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        # Change shape to (max_len, 1, d_model) so it can broadcast correctly over the batch dimension.\n",
    "        pe = pe.unsqueeze(1)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: Tensor of shape (seq_len, batch_size, d_model)\n",
    "        Returns:\n",
    "            x: Tensor with positional encoding added, shape (seq_len, batch_size, d_model)\n",
    "        \"\"\"\n",
    "        x = x + self.pe[:x.size(0)]\n",
    "        return x\n",
    "\n",
    "class MultilingualTransformer(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model=512, nhead=8, num_encoder_layers=6, \n",
    "                 num_decoder_layers=6, dim_feedforward=2048, dropout=0.1, max_seq_length=5000):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
    "        self.pos_encoder = PositionalEncoding(d_model, max_seq_length)\n",
    "        # Using PyTorch's Transformer module (it expects inputs in shape (seq_len, batch, d_model))\n",
    "        self.transformer = nn.Transformer(d_model, nhead, num_encoder_layers, num_decoder_layers, dim_feedforward, dropout)\n",
    "        self.fc_out = nn.Linear(d_model, vocab_size)\n",
    "        \n",
    "        # Parameter for language tag bias (can be used to steer attention toward target language tokens)\n",
    "        self.lang_tag_bias = nn.Parameter(torch.zeros(d_model))\n",
    "\n",
    "    def forward(self, src, tgt, src_mask=None, tgt_mask=None, src_key_padding_mask=None, tgt_key_padding_mask=None):\n",
    "        # src and tgt: shape (seq_len, batch_size)\n",
    "        src_emb = self.embedding(src) * math.sqrt(self.d_model)  # (src_len, batch, d_model)\n",
    "        tgt_emb = self.embedding(tgt) * math.sqrt(self.d_model)  # (tgt_len, batch, d_model)\n",
    "        \n",
    "        src_emb = self.pos_encoder(src_emb)\n",
    "        tgt_emb = self.pos_encoder(tgt_emb)\n",
    "        \n",
    "        # Inject language tag bias into the embeddings (a simple way to illustrate the idea)\n",
    "        src_emb = src_emb + self.lang_tag_bias\n",
    "        tgt_emb = tgt_emb + self.lang_tag_bias\n",
    "\n",
    "        # Pass through the transformer\n",
    "        output = self.transformer(src_emb, tgt_emb, src_mask=src_mask, tgt_mask=tgt_mask, \n",
    "                                  src_key_padding_mask=src_key_padding_mask, \n",
    "                                  tgt_key_padding_mask=tgt_key_padding_mask)\n",
    "        output = self.fc_out(output)  # (tgt_len, batch, vocab_size)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# train.py\n",
    "Training Script with Logging and Visualization + Aligning Embeddings\n",
    "\n",
    "### What This Code Does\n",
    "1. Model Initialization & Device Setup:\n",
    "    - It creates the MultilingualTransformer model, moves it to the proper device, and logs the status.\n",
    "\n",
    "2. Aligned Embeddings Integration:\n",
    "    - After the model is instantiated, the code:\n",
    "    - Loads the pretrained fastText model.\n",
    "    - Uses the load_aligned_embeddings function to compute an embedding matrix for the SentencePiece vocabulary.\n",
    "    - Copies these embeddings into the model’s embedding layer and (optionally) freezes them.\n",
    "\n",
    "3. Training Setup:\n",
    "    - The code then sets up the dataset, dataloader, loss function, optimizer, and training loop. It logs loss values and saves the training loss plot and model state.\n",
    "\n",
    "4. Running the Script:\n",
    "    - Run this updated train.py script. It will integrate the aligned embeddings before starting the training process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "# from nmt_model import MultilingualTransformer  # Ensure nmt_model.py is in the same directory\n",
    "import matplotlib.pyplot as plt\n",
    "import logging\n",
    "import sentencepiece as spm\n",
    "import numpy as np\n",
    "from fasttext import load_model  # for loading the fastText model\n",
    "import os\n",
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "# --------------------- Aligned Embeddings Functions --------------------- #\n",
    "def load_aligned_embeddings(sp_model_file, fasttext_model, alignment_matrix_path, vocab_size, d_model):\n",
    "    \"\"\"\n",
    "    Map tokens from your SentencePiece model to fastText embeddings,\n",
    "    apply the alignment transformation, and return a weight matrix of shape (vocab_size, d_model).\n",
    "    \"\"\"\n",
    "    sp = spm.SentencePieceProcessor(model_file=sp_model_file)\n",
    "    W = np.load(alignment_matrix_path)  # Expected shape: (d_model, d_model)\n",
    "    embedding_matrix = np.zeros((vocab_size, d_model))\n",
    "    for i in range(vocab_size):\n",
    "        token = sp.id_to_piece(i)\n",
    "        vector = fasttext_model.get_word_vector(token)  # Expected shape: (300,)\n",
    "        aligned_vector = np.dot(W, vector)             # Resulting shape: (300,)\n",
    "        embedding_matrix[i] = aligned_vector\n",
    "    return torch.tensor(embedding_matrix, dtype=torch.float)\n",
    "\n",
    "# --------------------- Dictionary Loader --------------------- #\n",
    "def load_dictionary(dict_path):\n",
    "    \"\"\"\n",
    "    Load a bilingual dictionary from a file.\n",
    "    Each line should have an English word and its Bengali translation separated by a tab.\n",
    "    Multi-word translations are preserved.\n",
    "    Returns a list of (english, bengali) pairs.\n",
    "    \"\"\"\n",
    "    dictionary = []\n",
    "    with open(dict_path, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            parts = line.strip().split('\\t', 1)\n",
    "            if len(parts) != 2:\n",
    "                continue\n",
    "            en_word = parts[0].strip()\n",
    "            bn_word = parts[1].strip()\n",
    "            dictionary.append((en_word, bn_word))\n",
    "    return dictionary\n",
    "\n",
    "# --------------------- Dataset and Collation --------------------- #\n",
    "class TokenizedDataset(Dataset):\n",
    "    def __init__(self, src_file, tgt_file, sp_model):\n",
    "        with open(src_file, 'r', encoding='utf-8') as f:\n",
    "            self.src_lines = f.readlines()\n",
    "        with open(tgt_file, 'r', encoding='utf-8') as f:\n",
    "            self.tgt_lines = f.readlines()\n",
    "        assert len(self.src_lines) == len(self.tgt_lines), \"Source and target files must have the same number of lines.\"\n",
    "        self.sp_model = sp_model\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.src_lines)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        src_ids = self.sp_model.encode_as_ids(self.src_lines[idx].strip())\n",
    "        tgt_ids = self.sp_model.encode_as_ids(self.tgt_lines[idx].strip())\n",
    "        return torch.tensor(src_ids, dtype=torch.long), torch.tensor(tgt_ids, dtype=torch.long)\n",
    "\n",
    "def collate_fn(batch):\n",
    "    src_batch, tgt_batch = zip(*batch)\n",
    "    max_src = max(len(seq) for seq in src_batch)\n",
    "    max_tgt = max(len(seq) for seq in tgt_batch)\n",
    "    padded_src = [torch.cat([seq, torch.zeros(max_src - len(seq), dtype=torch.long)]) for seq in src_batch]\n",
    "    padded_tgt = [torch.cat([seq, torch.zeros(max_tgt - len(seq), dtype=torch.long)]) for seq in tgt_batch]\n",
    "    padded_src = torch.stack(padded_src).transpose(0, 1)\n",
    "    padded_tgt = torch.stack(padded_tgt).transpose(0, 1)\n",
    "    return padded_src, padded_tgt\n",
    "\n",
    "# --------------------- Inference Function --------------------- #\n",
    "def greedy_decode(model, src, sp, max_len=50, start_token_id=1, end_token_id=0):\n",
    "    \"\"\"\n",
    "    Greedy decoding for a single source sentence.\n",
    "    Assumes src is a tensor of shape (seq_len, 1).\n",
    "    Returns a list of token ids.\n",
    "    \"\"\"\n",
    "    device = next(model.parameters()).device\n",
    "    src = src.to(device)\n",
    "    # Encode source sentence using the model's encoder\n",
    "    memory = model.transformer.encoder(model.pos_encoder(model.embedding(src)))\n",
    "    # Initialize ys with shape (1, batch)\n",
    "    ys = torch.tensor([[start_token_id]], dtype=torch.long, device=device)\n",
    "    for _ in range(max_len - 1):\n",
    "        tgt_mask = model.transformer.generate_square_subsequent_mask(ys.size(0)).to(device)\n",
    "        out = model.transformer.decoder(model.pos_encoder(model.embedding(ys)), memory, tgt_mask=tgt_mask)\n",
    "        out = model.fc_out(out[-1, :])\n",
    "        # next_word is computed with shape (batch,), then unsqueeze to (1, batch)\n",
    "        next_word = torch.argmax(out, dim=-1).unsqueeze(0)\n",
    "        if next_word.item() == end_token_id:\n",
    "            break\n",
    "        # Concatenate without unsqueezing further (so next_word remains shape (1, batch))\n",
    "        ys = torch.cat([ys, next_word], dim=0)\n",
    "    return ys.squeeze().tolist()\n",
    "\n",
    "def save_sample_translations(model, sp, sample_src_file, output_file, num_samples=10):\n",
    "    \"\"\"\n",
    "    Decode num_samples sentences from sample_src_file using greedy decoding\n",
    "    and save the source and generated translations for qualitative analysis.\n",
    "    \"\"\"\n",
    "    device = next(model.parameters()).device\n",
    "    with open(sample_src_file, 'r', encoding='utf-8') as f:\n",
    "        lines = f.readlines()\n",
    "    samples = lines[:num_samples]\n",
    "    results = []\n",
    "    for line in samples:\n",
    "        src_ids = sp.encode_as_ids(line.strip())\n",
    "        src_tensor = torch.tensor(src_ids, dtype=torch.long).unsqueeze(1)  # (seq_len, 1)\n",
    "        translation_ids = greedy_decode(model, src_tensor, sp)\n",
    "        translation = sp.decode_ids(translation_ids if isinstance(translation_ids, list) else [translation_ids])\n",
    "        results.append((line.strip(), translation))\n",
    "    \n",
    "    with open(output_file, 'w', encoding='utf-8') as f:\n",
    "        for src, trans in results:\n",
    "            f.write(\"Source: \" + src + \"\\n\")\n",
    "            f.write(\"Translation: \" + trans + \"\\n\\n\")\n",
    "    print(f\"Sample translations saved to {output_file}\")\n",
    "\n",
    "# --------------------- Training Function --------------------- #\n",
    "def train_model():\n",
    "    # Hyperparameters\n",
    "    vocab_size = 32000  # Must match your SentencePiece model\n",
    "    d_model = 300       # Set to 300 to match fastText dimensions\n",
    "    nhead = 6           # 300 is divisible by 6 (300/6 = 50)\n",
    "    num_encoder_layers = 3\n",
    "    num_decoder_layers = 3\n",
    "    num_epochs = 50\n",
    "    batch_size = 32\n",
    "    learning_rate = 1e-4\n",
    "    \n",
    "    # Initialize model (ensure MultilingualTransformer is defined in nmt_model.py)\n",
    "    model = MultilingualTransformer(vocab_size, d_model, nhead, num_encoder_layers, num_decoder_layers)\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model.to(device)\n",
    "    logging.info(\"Model initialized and moved to device: %s\", device)\n",
    "    \n",
    "    # ===== Integrate Aligned Embeddings =====\n",
    "    en_ft_model = load_model(\"english_fasttext_model.bin\")\n",
    "    aligned_embeds = load_aligned_embeddings(\"spm_model.model\", en_ft_model, \"alignment_matrix_en_to_bn.npy\", vocab_size, d_model)\n",
    "    model.embedding.weight.data.copy_(aligned_embeds)\n",
    "    model.embedding.weight.requires_grad = False  # Freeze embeddings\n",
    "    logging.info(\"Aligned embeddings integrated into the model.\")\n",
    "    # ===========================================\n",
    "    \n",
    "    criterion = nn.CrossEntropyLoss(ignore_index=0)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    \n",
    "    sp = spm.SentencePieceProcessor(model_file='spm_model.model')\n",
    "    train_dataset = TokenizedDataset('eng_tokenized.txt', 'ben_tokenized.txt', sp)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)\n",
    "    logging.info(\"Training data loaded. Total batches: %d\", len(train_loader))\n",
    "    \n",
    "    training_losses = []\n",
    "    \n",
    "    for epoch in range(1, num_epochs + 1):\n",
    "        model.train()\n",
    "        epoch_loss = 0.0\n",
    "        for batch_idx, (src, tgt) in enumerate(train_loader):\n",
    "            src, tgt = src.to(device), tgt.to(device)\n",
    "            tgt_input = tgt[:-1, :]   # Decoder input\n",
    "            tgt_output = tgt[1:, :]    # Expected output\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            tgt_mask = model.transformer.generate_square_subsequent_mask(tgt_input.size(0)).to(device)\n",
    "            output = model(src, tgt_input, tgt_mask=tgt_mask)\n",
    "            output = output.reshape(-1, vocab_size)\n",
    "            tgt_output = tgt_output.reshape(-1)\n",
    "            \n",
    "            loss = criterion(output, tgt_output)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            epoch_loss += loss.item()\n",
    "            if batch_idx % 10 == 0:\n",
    "                logging.info(\"Epoch %d, Batch %d, Loss: %.4f\", epoch, batch_idx, loss.item())\n",
    "        \n",
    "        avg_loss = epoch_loss / len(train_loader)\n",
    "        training_losses.append(avg_loss)\n",
    "        logging.info(\"Epoch %d complete, Average Loss: %.4f\", epoch, avg_loss)\n",
    "    \n",
    "    torch.save(model.state_dict(), \"multilingual_nmt_model.pt\")\n",
    "    logging.info(\"Model saved to multilingual_nmt_model.pt\")\n",
    "    \n",
    "    plt.figure(figsize=(8, 5))\n",
    "    plt.plot(range(1, num_epochs + 1), training_losses, marker='o')\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Average Loss\")\n",
    "    plt.title(\"Training Loss Over Epochs\")\n",
    "    plt.savefig(\"training_loss.png\")\n",
    "    plt.close()\n",
    "    logging.info(\"Training loss plot saved to training_loss.png\")\n",
    "    \n",
    "    # Save sample translations for qualitative evaluation\n",
    "    save_sample_translations(model, sp, 'eng_tokenized.txt', \"sample_translations.txt\", num_samples=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-17 15:24:57,442 - INFO - Model initialized and moved to device: cpu\n",
      "2025-02-17 15:25:23,763 - INFO - Aligned embeddings integrated into the model.\n",
      "2025-02-17 15:25:23,863 - INFO - Training data loaded. Total batches: 32\n",
      "2025-02-17 15:25:27,192 - INFO - Epoch 1, Batch 0, Loss: 10.5708\n",
      "2025-02-17 15:25:40,081 - INFO - Epoch 1, Batch 10, Loss: 9.0968\n",
      "2025-02-17 15:25:51,784 - INFO - Epoch 1, Batch 20, Loss: 8.6651\n",
      "2025-02-17 15:26:03,218 - INFO - Epoch 1, Batch 30, Loss: 8.1585\n",
      "2025-02-17 15:26:03,565 - INFO - Epoch 1 complete, Average Loss: 8.9165\n",
      "2025-02-17 15:26:04,709 - INFO - Epoch 2, Batch 0, Loss: 8.0415\n",
      "2025-02-17 15:26:15,665 - INFO - Epoch 2, Batch 10, Loss: 7.8244\n",
      "2025-02-17 15:26:26,539 - INFO - Epoch 2, Batch 20, Loss: 7.5513\n",
      "2025-02-17 15:26:37,662 - INFO - Epoch 2, Batch 30, Loss: 7.3505\n",
      "2025-02-17 15:26:37,927 - INFO - Epoch 2 complete, Average Loss: 7.6722\n",
      "2025-02-17 15:26:39,020 - INFO - Epoch 3, Batch 0, Loss: 7.2937\n",
      "2025-02-17 15:26:48,958 - INFO - Epoch 3, Batch 10, Loss: 7.1787\n",
      "2025-02-17 15:26:59,569 - INFO - Epoch 3, Batch 20, Loss: 7.1777\n",
      "2025-02-17 15:27:10,579 - INFO - Epoch 3, Batch 30, Loss: 6.8454\n",
      "2025-02-17 15:27:10,806 - INFO - Epoch 3 complete, Average Loss: 7.0497\n",
      "2025-02-17 15:27:11,872 - INFO - Epoch 4, Batch 0, Loss: 6.8559\n",
      "2025-02-17 15:27:24,112 - INFO - Epoch 4, Batch 10, Loss: 6.8027\n",
      "2025-02-17 15:27:33,839 - INFO - Epoch 4, Batch 20, Loss: 6.6957\n",
      "2025-02-17 15:27:43,502 - INFO - Epoch 4, Batch 30, Loss: 6.5364\n",
      "2025-02-17 15:27:43,782 - INFO - Epoch 4 complete, Average Loss: 6.7138\n",
      "2025-02-17 15:27:44,850 - INFO - Epoch 5, Batch 0, Loss: 6.5745\n",
      "2025-02-17 15:27:55,526 - INFO - Epoch 5, Batch 10, Loss: 6.4850\n",
      "2025-02-17 15:28:06,583 - INFO - Epoch 5, Batch 20, Loss: 6.6619\n",
      "2025-02-17 15:28:15,828 - INFO - Epoch 5, Batch 30, Loss: 6.6358\n",
      "2025-02-17 15:28:16,048 - INFO - Epoch 5 complete, Average Loss: 6.5154\n",
      "2025-02-17 15:28:17,017 - INFO - Epoch 6, Batch 0, Loss: 6.3733\n",
      "2025-02-17 15:28:27,773 - INFO - Epoch 6, Batch 10, Loss: 6.4281\n",
      "2025-02-17 15:28:37,224 - INFO - Epoch 6, Batch 20, Loss: 6.4738\n",
      "2025-02-17 15:28:47,453 - INFO - Epoch 6, Batch 30, Loss: 6.5990\n",
      "2025-02-17 15:28:47,732 - INFO - Epoch 6 complete, Average Loss: 6.4030\n",
      "2025-02-17 15:28:48,613 - INFO - Epoch 7, Batch 0, Loss: 6.3069\n",
      "2025-02-17 15:28:58,832 - INFO - Epoch 7, Batch 10, Loss: 6.4145\n",
      "2025-02-17 15:29:08,761 - INFO - Epoch 7, Batch 20, Loss: 6.2363\n",
      "2025-02-17 15:29:18,698 - INFO - Epoch 7, Batch 30, Loss: 6.2522\n",
      "2025-02-17 15:29:18,939 - INFO - Epoch 7 complete, Average Loss: 6.3351\n",
      "2025-02-17 15:29:19,881 - INFO - Epoch 8, Batch 0, Loss: 6.3587\n",
      "2025-02-17 15:29:30,136 - INFO - Epoch 8, Batch 10, Loss: 6.4117\n",
      "2025-02-17 15:29:40,116 - INFO - Epoch 8, Batch 20, Loss: 6.4355\n",
      "2025-02-17 15:29:49,351 - INFO - Epoch 8, Batch 30, Loss: 6.0362\n",
      "2025-02-17 15:29:49,566 - INFO - Epoch 8 complete, Average Loss: 6.2805\n",
      "2025-02-17 15:29:50,479 - INFO - Epoch 9, Batch 0, Loss: 6.2534\n",
      "2025-02-17 15:30:00,332 - INFO - Epoch 9, Batch 10, Loss: 6.1637\n",
      "2025-02-17 15:30:10,155 - INFO - Epoch 9, Batch 20, Loss: 6.1718\n",
      "2025-02-17 15:30:20,731 - INFO - Epoch 9, Batch 30, Loss: 6.2497\n",
      "2025-02-17 15:30:20,993 - INFO - Epoch 9 complete, Average Loss: 6.2327\n",
      "2025-02-17 15:30:21,980 - INFO - Epoch 10, Batch 0, Loss: 6.0913\n",
      "2025-02-17 15:30:33,997 - INFO - Epoch 10, Batch 10, Loss: 6.1661\n",
      "2025-02-17 15:30:44,415 - INFO - Epoch 10, Batch 20, Loss: 6.2459\n",
      "2025-02-17 15:30:54,031 - INFO - Epoch 10, Batch 30, Loss: 6.1247\n",
      "2025-02-17 15:30:54,264 - INFO - Epoch 10 complete, Average Loss: 6.1849\n",
      "2025-02-17 15:30:55,166 - INFO - Epoch 11, Batch 0, Loss: 6.0506\n",
      "2025-02-17 15:31:04,860 - INFO - Epoch 11, Batch 10, Loss: 6.1949\n",
      "2025-02-17 15:31:15,559 - INFO - Epoch 11, Batch 20, Loss: 6.1293\n",
      "2025-02-17 15:31:25,566 - INFO - Epoch 11, Batch 30, Loss: 5.9968\n",
      "2025-02-17 15:31:25,795 - INFO - Epoch 11 complete, Average Loss: 6.1339\n",
      "2025-02-17 15:31:26,943 - INFO - Epoch 12, Batch 0, Loss: 6.1920\n",
      "2025-02-17 15:31:36,674 - INFO - Epoch 12, Batch 10, Loss: 6.0937\n",
      "2025-02-17 15:31:46,860 - INFO - Epoch 12, Batch 20, Loss: 6.0448\n",
      "2025-02-17 15:31:56,976 - INFO - Epoch 12, Batch 30, Loss: 6.1539\n",
      "2025-02-17 15:31:57,225 - INFO - Epoch 12 complete, Average Loss: 6.0808\n",
      "2025-02-17 15:31:58,326 - INFO - Epoch 13, Batch 0, Loss: 5.9861\n",
      "2025-02-17 15:32:07,419 - INFO - Epoch 13, Batch 10, Loss: 5.8415\n",
      "2025-02-17 15:32:18,133 - INFO - Epoch 13, Batch 20, Loss: 6.1750\n",
      "2025-02-17 15:32:28,508 - INFO - Epoch 13, Batch 30, Loss: 5.9855\n",
      "2025-02-17 15:32:28,752 - INFO - Epoch 13 complete, Average Loss: 6.0169\n",
      "2025-02-17 15:32:29,620 - INFO - Epoch 14, Batch 0, Loss: 5.9505\n",
      "2025-02-17 15:32:39,308 - INFO - Epoch 14, Batch 10, Loss: 6.0447\n",
      "2025-02-17 15:32:48,698 - INFO - Epoch 14, Batch 20, Loss: 5.8226\n",
      "2025-02-17 15:32:59,155 - INFO - Epoch 14, Batch 30, Loss: 5.9933\n",
      "2025-02-17 15:32:59,384 - INFO - Epoch 14 complete, Average Loss: 5.9561\n",
      "2025-02-17 15:33:00,295 - INFO - Epoch 15, Batch 0, Loss: 5.9701\n",
      "2025-02-17 15:33:09,717 - INFO - Epoch 15, Batch 10, Loss: 5.8597\n",
      "2025-02-17 15:33:19,680 - INFO - Epoch 15, Batch 20, Loss: 6.0977\n",
      "2025-02-17 15:33:29,822 - INFO - Epoch 15, Batch 30, Loss: 5.7370\n",
      "2025-02-17 15:33:30,055 - INFO - Epoch 15 complete, Average Loss: 5.8912\n",
      "2025-02-17 15:33:31,182 - INFO - Epoch 16, Batch 0, Loss: 5.7895\n",
      "2025-02-17 15:33:41,113 - INFO - Epoch 16, Batch 10, Loss: 5.7416\n",
      "2025-02-17 15:33:50,651 - INFO - Epoch 16, Batch 20, Loss: 5.8439\n",
      "2025-02-17 15:34:00,415 - INFO - Epoch 16, Batch 30, Loss: 5.8884\n",
      "2025-02-17 15:34:00,625 - INFO - Epoch 16 complete, Average Loss: 5.8146\n",
      "2025-02-17 15:34:01,377 - INFO - Epoch 17, Batch 0, Loss: 5.7050\n",
      "2025-02-17 15:34:10,725 - INFO - Epoch 17, Batch 10, Loss: 5.7228\n",
      "2025-02-17 15:34:20,626 - INFO - Epoch 17, Batch 20, Loss: 5.8367\n",
      "2025-02-17 15:34:30,342 - INFO - Epoch 17, Batch 30, Loss: 5.7733\n",
      "2025-02-17 15:34:30,565 - INFO - Epoch 17 complete, Average Loss: 5.7367\n",
      "2025-02-17 15:34:31,454 - INFO - Epoch 18, Batch 0, Loss: 5.6660\n",
      "2025-02-17 15:34:41,144 - INFO - Epoch 18, Batch 10, Loss: 5.6147\n",
      "2025-02-17 15:34:50,362 - INFO - Epoch 18, Batch 20, Loss: 5.5079\n",
      "2025-02-17 15:35:00,880 - INFO - Epoch 18, Batch 30, Loss: 5.6723\n",
      "2025-02-17 15:35:01,093 - INFO - Epoch 18 complete, Average Loss: 5.6624\n",
      "2025-02-17 15:35:02,082 - INFO - Epoch 19, Batch 0, Loss: 5.6990\n",
      "2025-02-17 15:35:12,103 - INFO - Epoch 19, Batch 10, Loss: 5.5867\n",
      "2025-02-17 15:35:21,860 - INFO - Epoch 19, Batch 20, Loss: 5.3703\n",
      "2025-02-17 15:35:32,071 - INFO - Epoch 19, Batch 30, Loss: 5.7098\n",
      "2025-02-17 15:35:32,293 - INFO - Epoch 19 complete, Average Loss: 5.5766\n",
      "2025-02-17 15:35:33,425 - INFO - Epoch 20, Batch 0, Loss: 5.6418\n",
      "2025-02-17 15:35:42,923 - INFO - Epoch 20, Batch 10, Loss: 5.3538\n",
      "2025-02-17 15:35:53,723 - INFO - Epoch 20, Batch 20, Loss: 5.4946\n",
      "2025-02-17 15:36:03,014 - INFO - Epoch 20, Batch 30, Loss: 5.4747\n",
      "2025-02-17 15:36:03,231 - INFO - Epoch 20 complete, Average Loss: 5.5062\n",
      "2025-02-17 15:36:04,327 - INFO - Epoch 21, Batch 0, Loss: 5.6193\n",
      "2025-02-17 15:36:14,525 - INFO - Epoch 21, Batch 10, Loss: 5.3958\n",
      "2025-02-17 15:36:23,786 - INFO - Epoch 21, Batch 20, Loss: 5.3776\n",
      "2025-02-17 15:36:33,708 - INFO - Epoch 21, Batch 30, Loss: 5.3445\n",
      "2025-02-17 15:36:33,921 - INFO - Epoch 21 complete, Average Loss: 5.4170\n",
      "2025-02-17 15:36:34,904 - INFO - Epoch 22, Batch 0, Loss: 5.3366\n",
      "2025-02-17 15:36:45,100 - INFO - Epoch 22, Batch 10, Loss: 5.3364\n",
      "2025-02-17 15:36:55,254 - INFO - Epoch 22, Batch 20, Loss: 5.1906\n",
      "2025-02-17 15:37:04,468 - INFO - Epoch 22, Batch 30, Loss: 5.2881\n",
      "2025-02-17 15:37:04,687 - INFO - Epoch 22 complete, Average Loss: 5.3271\n",
      "2025-02-17 15:37:05,623 - INFO - Epoch 23, Batch 0, Loss: 5.1741\n",
      "2025-02-17 15:37:15,236 - INFO - Epoch 23, Batch 10, Loss: 5.1881\n",
      "2025-02-17 15:37:25,762 - INFO - Epoch 23, Batch 20, Loss: 5.2465\n",
      "2025-02-17 15:37:35,219 - INFO - Epoch 23, Batch 30, Loss: 5.2368\n",
      "2025-02-17 15:37:35,442 - INFO - Epoch 23 complete, Average Loss: 5.2394\n",
      "2025-02-17 15:37:36,405 - INFO - Epoch 24, Batch 0, Loss: 5.1695\n",
      "2025-02-17 15:37:46,840 - INFO - Epoch 24, Batch 10, Loss: 5.0648\n",
      "2025-02-17 15:37:56,127 - INFO - Epoch 24, Batch 20, Loss: 5.2521\n",
      "2025-02-17 15:38:05,775 - INFO - Epoch 24, Batch 30, Loss: 5.0345\n",
      "2025-02-17 15:38:05,997 - INFO - Epoch 24 complete, Average Loss: 5.1457\n",
      "2025-02-17 15:38:06,999 - INFO - Epoch 25, Batch 0, Loss: 5.0182\n",
      "2025-02-17 15:38:16,907 - INFO - Epoch 25, Batch 10, Loss: 5.0495\n",
      "2025-02-17 15:38:26,272 - INFO - Epoch 25, Batch 20, Loss: 5.1217\n",
      "2025-02-17 15:38:36,201 - INFO - Epoch 25, Batch 30, Loss: 5.0309\n",
      "2025-02-17 15:38:36,419 - INFO - Epoch 25 complete, Average Loss: 5.0499\n",
      "2025-02-17 15:38:37,269 - INFO - Epoch 26, Batch 0, Loss: 4.8172\n",
      "2025-02-17 15:38:47,217 - INFO - Epoch 26, Batch 10, Loss: 4.9419\n",
      "2025-02-17 15:38:57,033 - INFO - Epoch 26, Batch 20, Loss: 5.0145\n",
      "2025-02-17 15:39:06,881 - INFO - Epoch 26, Batch 30, Loss: 5.0882\n",
      "2025-02-17 15:39:07,119 - INFO - Epoch 26 complete, Average Loss: 4.9532\n",
      "2025-02-17 15:39:07,909 - INFO - Epoch 27, Batch 0, Loss: 4.8775\n",
      "2025-02-17 15:39:17,730 - INFO - Epoch 27, Batch 10, Loss: 4.9458\n",
      "2025-02-17 15:39:26,959 - INFO - Epoch 27, Batch 20, Loss: 4.7203\n",
      "2025-02-17 15:39:37,480 - INFO - Epoch 27, Batch 30, Loss: 4.9476\n",
      "2025-02-17 15:39:37,671 - INFO - Epoch 27 complete, Average Loss: 4.8723\n",
      "2025-02-17 15:39:38,475 - INFO - Epoch 28, Batch 0, Loss: 4.6293\n",
      "2025-02-17 15:39:48,971 - INFO - Epoch 28, Batch 10, Loss: 4.8688\n",
      "2025-02-17 15:39:58,141 - INFO - Epoch 28, Batch 20, Loss: 4.7157\n",
      "2025-02-17 15:40:08,081 - INFO - Epoch 28, Batch 30, Loss: 4.7439\n",
      "2025-02-17 15:40:08,296 - INFO - Epoch 28 complete, Average Loss: 4.7644\n",
      "2025-02-17 15:40:09,160 - INFO - Epoch 29, Batch 0, Loss: 4.5994\n",
      "2025-02-17 15:40:19,340 - INFO - Epoch 29, Batch 10, Loss: 4.6582\n",
      "2025-02-17 15:40:28,698 - INFO - Epoch 29, Batch 20, Loss: 4.6643\n",
      "2025-02-17 15:40:39,201 - INFO - Epoch 29, Batch 30, Loss: 4.5945\n",
      "2025-02-17 15:40:39,413 - INFO - Epoch 29 complete, Average Loss: 4.6708\n",
      "2025-02-17 15:40:40,308 - INFO - Epoch 30, Batch 0, Loss: 4.5640\n",
      "2025-02-17 15:40:50,363 - INFO - Epoch 30, Batch 10, Loss: 4.5744\n",
      "2025-02-17 15:41:00,521 - INFO - Epoch 30, Batch 20, Loss: 4.4293\n",
      "2025-02-17 15:41:09,650 - INFO - Epoch 30, Batch 30, Loss: 4.6188\n",
      "2025-02-17 15:41:09,944 - INFO - Epoch 30 complete, Average Loss: 4.5775\n",
      "2025-02-17 15:41:11,055 - INFO - Epoch 31, Batch 0, Loss: 4.4809\n",
      "2025-02-17 15:41:21,050 - INFO - Epoch 31, Batch 10, Loss: 4.4609\n",
      "2025-02-17 15:41:31,159 - INFO - Epoch 31, Batch 20, Loss: 4.3560\n",
      "2025-02-17 15:41:40,180 - INFO - Epoch 31, Batch 30, Loss: 4.4144\n",
      "2025-02-17 15:41:40,385 - INFO - Epoch 31 complete, Average Loss: 4.4804\n",
      "2025-02-17 15:41:41,442 - INFO - Epoch 32, Batch 0, Loss: 4.3175\n",
      "2025-02-17 15:41:51,250 - INFO - Epoch 32, Batch 10, Loss: 4.3570\n",
      "2025-02-17 15:42:02,058 - INFO - Epoch 32, Batch 20, Loss: 4.3620\n",
      "2025-02-17 15:42:13,131 - INFO - Epoch 32, Batch 30, Loss: 4.4615\n",
      "2025-02-17 15:42:13,365 - INFO - Epoch 32 complete, Average Loss: 4.3878\n",
      "2025-02-17 15:42:14,392 - INFO - Epoch 33, Batch 0, Loss: 4.4533\n",
      "2025-02-17 15:42:24,049 - INFO - Epoch 33, Batch 10, Loss: 4.3101\n",
      "2025-02-17 15:42:33,608 - INFO - Epoch 33, Batch 20, Loss: 4.2249\n",
      "2025-02-17 15:42:43,329 - INFO - Epoch 33, Batch 30, Loss: 4.3648\n",
      "2025-02-17 15:42:43,554 - INFO - Epoch 33 complete, Average Loss: 4.2840\n",
      "2025-02-17 15:42:44,501 - INFO - Epoch 34, Batch 0, Loss: 4.2826\n",
      "2025-02-17 15:42:53,892 - INFO - Epoch 34, Batch 10, Loss: 4.1638\n",
      "2025-02-17 15:43:03,195 - INFO - Epoch 34, Batch 20, Loss: 4.2615\n",
      "2025-02-17 15:43:13,733 - INFO - Epoch 34, Batch 30, Loss: 4.1732\n",
      "2025-02-17 15:43:13,956 - INFO - Epoch 34 complete, Average Loss: 4.1987\n",
      "2025-02-17 15:43:15,002 - INFO - Epoch 35, Batch 0, Loss: 4.2021\n",
      "2025-02-17 15:43:24,662 - INFO - Epoch 35, Batch 10, Loss: 4.0586\n",
      "2025-02-17 15:43:35,031 - INFO - Epoch 35, Batch 20, Loss: 4.0821\n",
      "2025-02-17 15:43:44,444 - INFO - Epoch 35, Batch 30, Loss: 3.9813\n",
      "2025-02-17 15:43:44,660 - INFO - Epoch 35 complete, Average Loss: 4.0891\n",
      "2025-02-17 15:43:45,572 - INFO - Epoch 36, Batch 0, Loss: 3.8457\n",
      "2025-02-17 15:43:55,024 - INFO - Epoch 36, Batch 10, Loss: 3.9930\n",
      "2025-02-17 15:44:04,952 - INFO - Epoch 36, Batch 20, Loss: 3.8971\n",
      "2025-02-17 15:44:14,921 - INFO - Epoch 36, Batch 30, Loss: 3.9720\n",
      "2025-02-17 15:44:15,146 - INFO - Epoch 36 complete, Average Loss: 3.9963\n",
      "2025-02-17 15:44:16,078 - INFO - Epoch 37, Batch 0, Loss: 3.9144\n",
      "2025-02-17 15:44:25,974 - INFO - Epoch 37, Batch 10, Loss: 3.8813\n",
      "2025-02-17 15:44:35,844 - INFO - Epoch 37, Batch 20, Loss: 3.8729\n",
      "2025-02-17 15:44:45,746 - INFO - Epoch 37, Batch 30, Loss: 3.9992\n",
      "2025-02-17 15:44:45,979 - INFO - Epoch 37 complete, Average Loss: 3.9059\n",
      "2025-02-17 15:44:47,148 - INFO - Epoch 38, Batch 0, Loss: 3.8037\n",
      "2025-02-17 15:44:56,839 - INFO - Epoch 38, Batch 10, Loss: 3.9332\n",
      "2025-02-17 15:45:06,489 - INFO - Epoch 38, Batch 20, Loss: 3.7437\n",
      "2025-02-17 15:45:16,367 - INFO - Epoch 38, Batch 30, Loss: 3.8776\n",
      "2025-02-17 15:45:16,544 - INFO - Epoch 38 complete, Average Loss: 3.7869\n",
      "2025-02-17 15:45:17,553 - INFO - Epoch 39, Batch 0, Loss: 3.6185\n",
      "2025-02-17 15:45:27,898 - INFO - Epoch 39, Batch 10, Loss: 3.7295\n",
      "2025-02-17 15:45:37,789 - INFO - Epoch 39, Batch 20, Loss: 3.6652\n",
      "2025-02-17 15:45:47,730 - INFO - Epoch 39, Batch 30, Loss: 3.7562\n",
      "2025-02-17 15:45:47,948 - INFO - Epoch 39 complete, Average Loss: 3.7047\n",
      "2025-02-17 15:45:48,976 - INFO - Epoch 40, Batch 0, Loss: 3.6787\n",
      "2025-02-17 15:45:58,493 - INFO - Epoch 40, Batch 10, Loss: 3.5636\n",
      "2025-02-17 15:46:08,565 - INFO - Epoch 40, Batch 20, Loss: 3.6815\n",
      "2025-02-17 15:46:18,005 - INFO - Epoch 40, Batch 30, Loss: 3.7091\n",
      "2025-02-17 15:46:18,236 - INFO - Epoch 40 complete, Average Loss: 3.6016\n",
      "2025-02-17 15:46:19,126 - INFO - Epoch 41, Batch 0, Loss: 3.5011\n",
      "2025-02-17 15:46:28,786 - INFO - Epoch 41, Batch 10, Loss: 3.4987\n",
      "2025-02-17 15:46:38,994 - INFO - Epoch 41, Batch 20, Loss: 3.5502\n",
      "2025-02-17 15:46:48,801 - INFO - Epoch 41, Batch 30, Loss: 3.5177\n",
      "2025-02-17 15:46:49,034 - INFO - Epoch 41 complete, Average Loss: 3.5050\n",
      "2025-02-17 15:46:49,991 - INFO - Epoch 42, Batch 0, Loss: 3.4841\n",
      "2025-02-17 15:46:59,851 - INFO - Epoch 42, Batch 10, Loss: 3.3792\n",
      "2025-02-17 15:47:10,107 - INFO - Epoch 42, Batch 20, Loss: 3.4394\n",
      "2025-02-17 15:47:20,744 - INFO - Epoch 42, Batch 30, Loss: 3.3207\n",
      "2025-02-17 15:47:20,966 - INFO - Epoch 42 complete, Average Loss: 3.3971\n",
      "2025-02-17 15:47:22,034 - INFO - Epoch 43, Batch 0, Loss: 3.3320\n",
      "2025-02-17 15:47:31,745 - INFO - Epoch 43, Batch 10, Loss: 3.2198\n",
      "2025-02-17 15:47:41,760 - INFO - Epoch 43, Batch 20, Loss: 3.3468\n",
      "2025-02-17 15:47:51,853 - INFO - Epoch 43, Batch 30, Loss: 3.2714\n",
      "2025-02-17 15:47:52,111 - INFO - Epoch 43 complete, Average Loss: 3.3004\n",
      "2025-02-17 15:47:53,398 - INFO - Epoch 44, Batch 0, Loss: 3.1879\n",
      "2025-02-17 15:48:02,909 - INFO - Epoch 44, Batch 10, Loss: 3.2026\n",
      "2025-02-17 15:48:12,516 - INFO - Epoch 44, Batch 20, Loss: 3.2995\n",
      "2025-02-17 15:48:22,423 - INFO - Epoch 44, Batch 30, Loss: 3.1383\n",
      "2025-02-17 15:48:22,648 - INFO - Epoch 44 complete, Average Loss: 3.2140\n",
      "2025-02-17 15:48:23,666 - INFO - Epoch 45, Batch 0, Loss: 3.1295\n",
      "2025-02-17 15:48:33,935 - INFO - Epoch 45, Batch 10, Loss: 3.1768\n",
      "2025-02-17 15:48:43,035 - INFO - Epoch 45, Batch 20, Loss: 3.0764\n",
      "2025-02-17 15:48:52,865 - INFO - Epoch 45, Batch 30, Loss: 3.1759\n",
      "2025-02-17 15:48:53,046 - INFO - Epoch 45 complete, Average Loss: 3.1180\n",
      "2025-02-17 15:48:53,983 - INFO - Epoch 46, Batch 0, Loss: 3.1828\n",
      "2025-02-17 15:49:04,045 - INFO - Epoch 46, Batch 10, Loss: 3.1281\n",
      "2025-02-17 15:49:13,481 - INFO - Epoch 46, Batch 20, Loss: 2.9849\n",
      "2025-02-17 15:49:23,587 - INFO - Epoch 46, Batch 30, Loss: 3.0118\n",
      "2025-02-17 15:49:23,802 - INFO - Epoch 46 complete, Average Loss: 3.0424\n",
      "2025-02-17 15:49:24,729 - INFO - Epoch 47, Batch 0, Loss: 2.9710\n",
      "2025-02-17 15:49:34,768 - INFO - Epoch 47, Batch 10, Loss: 2.9267\n",
      "2025-02-17 15:49:43,932 - INFO - Epoch 47, Batch 20, Loss: 2.9102\n",
      "2025-02-17 15:49:54,032 - INFO - Epoch 47, Batch 30, Loss: 2.9392\n",
      "2025-02-17 15:49:54,270 - INFO - Epoch 47 complete, Average Loss: 2.9265\n",
      "2025-02-17 15:49:55,449 - INFO - Epoch 48, Batch 0, Loss: 2.8670\n",
      "2025-02-17 15:50:05,529 - INFO - Epoch 48, Batch 10, Loss: 2.6411\n",
      "2025-02-17 15:50:15,261 - INFO - Epoch 48, Batch 20, Loss: 2.7783\n",
      "2025-02-17 15:50:24,734 - INFO - Epoch 48, Batch 30, Loss: 2.7878\n",
      "2025-02-17 15:50:24,952 - INFO - Epoch 48 complete, Average Loss: 2.8227\n",
      "2025-02-17 15:50:25,871 - INFO - Epoch 49, Batch 0, Loss: 2.6982\n",
      "2025-02-17 15:50:35,593 - INFO - Epoch 49, Batch 10, Loss: 2.7594\n",
      "2025-02-17 15:50:46,125 - INFO - Epoch 49, Batch 20, Loss: 2.7993\n",
      "2025-02-17 15:50:55,721 - INFO - Epoch 49, Batch 30, Loss: 2.7550\n",
      "2025-02-17 15:50:55,942 - INFO - Epoch 49 complete, Average Loss: 2.7392\n",
      "2025-02-17 15:50:57,115 - INFO - Epoch 50, Batch 0, Loss: 2.5285\n",
      "2025-02-17 15:51:06,935 - INFO - Epoch 50, Batch 10, Loss: 2.6592\n",
      "2025-02-17 15:51:15,762 - INFO - Epoch 50, Batch 20, Loss: 2.5239\n",
      "2025-02-17 15:51:25,888 - INFO - Epoch 50, Batch 30, Loss: 2.6401\n",
      "2025-02-17 15:51:26,141 - INFO - Epoch 50 complete, Average Loss: 2.6578\n",
      "2025-02-17 15:51:26,337 - INFO - Model saved to multilingual_nmt_model.pt\n",
      "2025-02-17 15:51:26,509 - INFO - Training loss plot saved to training_loss.png\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample translations saved to sample_translations.txt\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    train_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading fastText models...\n",
      "Loaded dictionary with 520 word pairs.\n",
      "Computing alignment matrix...\n",
      "Alignment matrix saved to alignment_matrix_en_to_bn.npy\n",
      "Cosine similarity histogram saved to visualizations\\cosine_similarity_histogram.png\n",
      "Singular values plot saved to visualizations\\singular_values.png\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    align_main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# test_making.py\n",
    "\n",
    "Creating a sample test file (in order to evaluate) from flores dataset for English Bengali."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created test set with 1000 lines in 'eng_test.txt' and 'ben_test.txt'.\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "def create_test_set(src_file, tgt_file, test_src_file, test_tgt_file, num_lines=1000):\n",
    "    \"\"\"\n",
    "    Extracts the first num_lines from the source and target files and saves them as test sets.\n",
    "    \n",
    "    Parameters:\n",
    "      src_file (str): Path to the English FLORES file.\n",
    "      tgt_file (str): Path to the Bengali FLORES file.\n",
    "      test_src_file (str): Output file for the test English sentences.\n",
    "      test_tgt_file (str): Output file for the test Bengali sentences.\n",
    "      num_lines (int): Number of sentence pairs to extract.\n",
    "    \"\"\"\n",
    "    with open(src_file, 'r', encoding='utf-8') as f_src, open(tgt_file, 'r', encoding='utf-8') as f_tgt:\n",
    "        src_lines = f_src.readlines()\n",
    "        tgt_lines = f_tgt.readlines()\n",
    "    \n",
    "    # Check that the files have the same number of lines.\n",
    "    if len(src_lines) != len(tgt_lines):\n",
    "        raise ValueError(\"Source and target files do not have the same number of lines.\")\n",
    "    \n",
    "    # Select the first num_lines as test set.\n",
    "    test_src = src_lines[:num_lines]\n",
    "    test_tgt = tgt_lines[:num_lines]\n",
    "    \n",
    "    with open(test_src_file, 'w', encoding='utf-8') as f_test_src:\n",
    "        f_test_src.writelines(test_src)\n",
    "    with open(test_tgt_file, 'w', encoding='utf-8') as f_test_tgt:\n",
    "        f_test_tgt.writelines(test_tgt)\n",
    "    \n",
    "    print(f\"Created test set with {num_lines} lines in '{test_src_file}' and '{test_tgt_file}'.\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # Update these filenames as needed based on your FLORES dataset files.\n",
    "    create_test_set('eng.dev', 'ben.dev', 'eng_test.txt', 'ben_test.txt', num_lines=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# eval_nmt.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "import torch\n",
    "import sentencepiece as spm\n",
    "import sacrebleu\n",
    "import logging\n",
    "from fasttext import load_model\n",
    "# from nmt_model import MultilingualTransformer  # Ensure this file is in the same directory\n",
    "import numpy as np\n",
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "# Greedy decoding function (as defined earlier)\n",
    "def greedy_decode(model, src, sp, max_len=50, start_token_id=1, end_token_id=0):\n",
    "    \"\"\"\n",
    "    Greedy decoding for a single source sentence.\n",
    "    Assumes src is a tensor of shape (seq_len, 1).\n",
    "    Returns a list of token ids.\n",
    "    \"\"\"\n",
    "    device = next(model.parameters()).device\n",
    "    src = src.to(device)\n",
    "    memory = model.transformer.encoder(model.pos_encoder(model.embedding(src)))\n",
    "    ys = torch.tensor([[start_token_id]], dtype=torch.long, device=device)\n",
    "    for _ in range(max_len - 1):\n",
    "        tgt_mask = model.transformer.generate_square_subsequent_mask(ys.size(0)).to(device)\n",
    "        out = model.transformer.decoder(model.pos_encoder(model.embedding(ys)), memory, tgt_mask=tgt_mask)\n",
    "        out = model.fc_out(out[-1, :])\n",
    "        next_word = torch.argmax(out, dim=-1).unsqueeze(0)\n",
    "        if next_word.item() == end_token_id:\n",
    "            break\n",
    "        ys = torch.cat([ys, next_word], dim=0)\n",
    "    return ys.squeeze().tolist()\n",
    "\n",
    "def load_test_data(src_path, ref_path):\n",
    "    \"\"\"\n",
    "    Load test data from the provided paths.\n",
    "    Returns a list of source sentences and a list of reference translations.\n",
    "    \"\"\"\n",
    "    with open(src_path, 'r', encoding='utf-8') as f:\n",
    "        src_sentences = [line.strip() for line in f if line.strip()]\n",
    "    with open(ref_path, 'r', encoding='utf-8') as f:\n",
    "        # Assumes one reference per line; sacreBLEU accepts list of references (one or more)\n",
    "        ref_sentences = [line.strip() for line in f if line.strip()]\n",
    "    return src_sentences, ref_sentences\n",
    "\n",
    "def generate_translations(model, sp, src_sentences):\n",
    "    \"\"\"\n",
    "    Generate translations for a list of source sentences using greedy decoding.\n",
    "    Returns a list of generated translations (decoded as strings).\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    translations = []\n",
    "    for sentence in src_sentences:\n",
    "        src_ids = sp.encode_as_ids(sentence)\n",
    "        src_tensor = torch.tensor(src_ids, dtype=torch.long).unsqueeze(1)  # (seq_len, 1)\n",
    "        translation_ids = greedy_decode(model, src_tensor, sp)\n",
    "        translation = sp.decode_ids(translation_ids if isinstance(translation_ids, list) else [translation_ids])\n",
    "        translations.append(translation)\n",
    "    return translations\n",
    "\n",
    "def evaluate_model():\n",
    "    # Paths to files and models\n",
    "    test_src_path = \"eng_test.txt\"  # Test set English sentences\n",
    "    test_ref_path = \"ben_test.txt\"  # Corresponding reference Bengali translations\n",
    "    sp_model_path = \"spm_model.model\"\n",
    "    model_path = \"multilingual_nmt_model.pt\"\n",
    "    \n",
    "    # Hyperparameters should match those used during training\n",
    "    vocab_size = 32000\n",
    "    d_model = 300\n",
    "    nhead = 6\n",
    "    num_encoder_layers = 3\n",
    "    num_decoder_layers = 3\n",
    "    \n",
    "    # Load the SentencePiece model\n",
    "    sp = spm.SentencePieceProcessor(model_file=sp_model_path)\n",
    "    \n",
    "    # Initialize and load the trained model\n",
    "    model = MultilingualTransformer(vocab_size, d_model, nhead, num_encoder_layers, num_decoder_layers)\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model.to(device)\n",
    "    model.load_state_dict(torch.load(model_path, map_location=device))\n",
    "    logging.info(\"Trained model loaded from %s\", model_path)\n",
    "    \n",
    "    # Load test data\n",
    "    src_sentences, ref_sentences = load_test_data(test_src_path, test_ref_path)\n",
    "    logging.info(\"Loaded %d test sentences.\", len(src_sentences))\n",
    "    \n",
    "    # Generate translations\n",
    "    generated_translations = generate_translations(model, sp, src_sentences)\n",
    "    \n",
    "    # Save generated translations for qualitative analysis\n",
    "    with open(\"generated_translations.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "        for src, trans in zip(src_sentences, generated_translations):\n",
    "            f.write(\"Source: \" + src + \"\\n\")\n",
    "            f.write(\"Translation: \" + trans + \"\\n\\n\")\n",
    "    logging.info(\"Generated translations saved to generated_translations.txt\")\n",
    "    \n",
    "    # Compute BLEU score using sacreBLEU\n",
    "    # sacreBLEU expects references as a list of lists\n",
    "    bleu = sacrebleu.corpus_bleu(generated_translations, [ref_sentences])\n",
    "    print(\"BLEU score:\", bleu.score)\n",
    "    logging.info(\"BLEU score: %.2f\", bleu.score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-17 11:03:22,118 - INFO - Trained model loaded from multilingual_nmt_model.pt\n",
      "2025-02-17 11:03:22,134 - INFO - Loaded 997 test sentences.\n",
      "2025-02-17 11:11:29,028 - INFO - Generated translations saved to generated_translations.txt\n",
      "2025-02-17 11:11:29,149 - INFO - BLEU score: 0.00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU score: 0.0\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    evaluate_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# eval_nmt_2.py\n",
    "\n",
    "Evaluation attempt 2, & this time:\n",
    "- Updated version of your inference code that uses beam search instead of greedy decoding.\n",
    "\n",
    "This code loads the trained multilingual NMT model and SentencePiece model, then uses beam search decoding to generate translations for a test set of English sentences. \n",
    "\n",
    "It saves these translations for inspection and computes an overall BLEU score to quantitatively evaluate translation quality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-17 11:33:02,161 - INFO - Trained model loaded from multilingual_nmt_model.pt\n",
      "2025-02-17 11:33:02,165 - INFO - Loaded 997 test sentences.\n",
      "2025-02-17 11:58:20,381 - INFO - Generated translations saved to generated_translations_beam.txt\n",
      "2025-02-17 11:58:20,493 - INFO - BLEU score: 0.00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU score: 0.0\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "import torch\n",
    "import sentencepiece as spm\n",
    "import sacrebleu\n",
    "import logging\n",
    "from fasttext import load_model\n",
    "# from nmt_model import MultilingualTransformer  # Ensure this file is in the same directory\n",
    "import os\n",
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "def beam_search_decode(model, src, sp, beam_width=3, max_len=50, start_token_id=1, end_token_id=0):\n",
    "    \"\"\"\n",
    "    Beam search decoding for a single source sentence.\n",
    "    Assumes src is a tensor of shape (seq_len, 1).\n",
    "    Returns the best candidate sequence as a list of token ids.\n",
    "    \"\"\"\n",
    "    device = next(model.parameters()).device\n",
    "    src = src.to(device)\n",
    "    memory = model.transformer.encoder(model.pos_encoder(model.embedding(src)))\n",
    "    # Initialize the beam with the start token. Ensure it is a 1D tensor.\n",
    "    beam = [(torch.tensor([start_token_id], dtype=torch.long, device=device), 0.0)]\n",
    "    \n",
    "    for _ in range(max_len - 1):\n",
    "        new_beam = []\n",
    "        for seq, score in beam:\n",
    "            seq = seq.view(-1)  # Ensure seq is 1D\n",
    "            if seq[-1].item() == end_token_id:\n",
    "                new_beam.append((seq, score))\n",
    "                continue\n",
    "            seq_input = seq.unsqueeze(1)  # shape: (current_seq_len, 1)\n",
    "            tgt_mask = model.transformer.generate_square_subsequent_mask(seq_input.size(0)).to(device)\n",
    "            out = model.transformer.decoder(model.pos_encoder(model.embedding(seq_input)), memory, tgt_mask=tgt_mask)\n",
    "            logits = model.fc_out(out[-1, :])  # shape: (batch, vocab_size), batch==1 so shape is (1, vocab_size)\n",
    "            log_probs = torch.log_softmax(logits, dim=-1)  # shape: (1, vocab_size)\n",
    "            # Get top beam_width candidates. Now topk_* will have shape (1, beam_width)\n",
    "            topk_log_probs, topk_indices = torch.topk(log_probs, beam_width, dim=-1)\n",
    "            # Iterate over beam_width candidates in the first row.\n",
    "            for j in range(beam_width):\n",
    "                candidate = topk_indices[0, j].unsqueeze(0)  # shape: (1,)\n",
    "                candidate_log_prob = topk_log_probs[0, j].item()\n",
    "                new_seq = torch.cat([seq, candidate], dim=0)\n",
    "                new_score = score + candidate_log_prob\n",
    "                new_beam.append((new_seq, new_score))\n",
    "        # Keep only the top beam_width candidates\n",
    "        beam = sorted(new_beam, key=lambda x: x[1], reverse=True)[:beam_width]\n",
    "        if all(seq[-1].item() == end_token_id for seq, _ in beam):\n",
    "            break\n",
    "    best_seq, best_score = beam[0]\n",
    "    return best_seq.squeeze().tolist()\n",
    "\n",
    "def generate_translations_beam(model, sp, src_sentences, beam_width=3):\n",
    "    model.eval()\n",
    "    translations = []\n",
    "    for sentence in src_sentences:\n",
    "        src_ids = sp.encode_as_ids(sentence.strip())\n",
    "        src_tensor = torch.tensor(src_ids, dtype=torch.long).unsqueeze(1)\n",
    "        translation_ids = beam_search_decode(model, src_tensor, sp, beam_width=beam_width)\n",
    "        translation = sp.decode_ids(translation_ids if isinstance(translation_ids, list) else [translation_ids])\n",
    "        translations.append(translation)\n",
    "    return translations\n",
    "\n",
    "def load_test_data(src_path, ref_path):\n",
    "    with open(src_path, 'r', encoding='utf-8') as f:\n",
    "        src_sentences = [line.strip() for line in f if line.strip()]\n",
    "    with open(ref_path, 'r', encoding='utf-8') as f:\n",
    "        ref_sentences = [line.strip() for line in f if line.strip()]\n",
    "    return src_sentences, ref_sentences\n",
    "\n",
    "def evaluate_model():\n",
    "    test_src_path = \"eng_test.txt\"    # Test set English sentences\n",
    "    test_ref_path = \"ben_test.txt\"     # Corresponding Bengali references\n",
    "    sp_model_path = \"spm_model.model\"\n",
    "    model_path = \"multilingual_nmt_model.pt\"\n",
    "    \n",
    "    vocab_size = 32000\n",
    "    d_model = 300\n",
    "    nhead = 6\n",
    "    num_encoder_layers = 3\n",
    "    num_decoder_layers = 3\n",
    "    \n",
    "    sp = spm.SentencePieceProcessor(model_file=sp_model_path)\n",
    "    model = MultilingualTransformer(vocab_size, d_model, nhead, num_encoder_layers, num_decoder_layers)\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model.to(device)\n",
    "    model.load_state_dict(torch.load(model_path, map_location=device))\n",
    "    logging.info(\"Trained model loaded from %s\", model_path)\n",
    "    \n",
    "    src_sentences, ref_sentences = load_test_data(\"eng_test.txt\", \"ben_test.txt\")\n",
    "    logging.info(\"Loaded %d test sentences.\", len(src_sentences))\n",
    "    \n",
    "    generated_translations = generate_translations_beam(model, sp, src_sentences, beam_width=3)\n",
    "    \n",
    "    with open(\"generated_translations_beam.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "        for src, trans in zip(src_sentences, generated_translations):\n",
    "            f.write(\"Source: \" + src + \"\\n\")\n",
    "            f.write(\"Translation: \" + trans + \"\\n\\n\")\n",
    "    logging.info(\"Generated translations saved to generated_translations_beam.txt\")\n",
    "    \n",
    "    bleu = sacrebleu.corpus_bleu(generated_translations, [ref_sentences])\n",
    "    logging.info(\"BLEU score: %.2f\", bleu.score)\n",
    "    print(\"BLEU score:\", bleu.score)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    evaluate_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# eval_nmt_3.py\n",
    "\n",
    "Beam Search Decoding:\n",
    "- The beam_search_decode function uses beam search to decode a source sentence, exploring multiple candidate sequences and returning the best one based on cumulative log-probabilities.\n",
    "\n",
    "Translation Generation:\n",
    "- The generate_translations_beam function applies beam search decoding to each source sentence in your test set and produces the final translations.\n",
    "\n",
    "Test Data Loading:\n",
    "- load_test_data reads your eng_test.txt and ben_test.txt files, assuming each line contains one sentence.\n",
    "\n",
    "Model Evaluation:\n",
    "- In evaluate_model, the script:\n",
    "    - Loads your SentencePiece model and your trained NMT model.\n",
    "    - Loads the test sentences.\n",
    "    - Generates translations for these sentences.\n",
    "    - Saves the generated translations for qualitative analysis.\n",
    "    - Computes the corpus BLEU score using sacreBLEU, and prints/logs it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-17 16:42:22,418 - INFO - Trained model loaded from multilingual_nmt_model.pt\n",
      "2025-02-17 16:42:22,421 - INFO - Loaded 100 test sentences.\n",
      "2025-02-17 16:44:42,415 - INFO - Generated translations saved to generated_translations_beam.txt\n",
      "2025-02-17 16:44:42,442 - INFO - BLEU score: 0.18\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU score: 0.1772453131944549\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "import torch\n",
    "import sentencepiece as spm\n",
    "import sacrebleu\n",
    "import logging\n",
    "from fasttext import load_model\n",
    "# from nmt_model import MultilingualTransformer  # Ensure this file is in your working directory\n",
    "import os\n",
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "def beam_search_decode(model, src, sp, beam_width=3, max_len=50, start_token_id=1, end_token_id=0):\n",
    "    \"\"\"\n",
    "    Beam search decoding for a single source sentence.\n",
    "    Assumes src is a tensor of shape (seq_len, 1).\n",
    "    Returns the best candidate sequence as a list of token ids.\n",
    "    \"\"\"\n",
    "    device = next(model.parameters()).device\n",
    "    src = src.to(device)\n",
    "    memory = model.transformer.encoder(model.pos_encoder(model.embedding(src)))\n",
    "    beam = [(torch.tensor([start_token_id], dtype=torch.long, device=device), 0.0)]\n",
    "    for _ in range(max_len - 1):\n",
    "        new_beam = []\n",
    "        for seq, score in beam:\n",
    "            seq = seq.view(-1)\n",
    "            if seq[-1].item() == end_token_id:\n",
    "                new_beam.append((seq, score))\n",
    "                continue\n",
    "            seq_input = seq.unsqueeze(1)  # (current_seq_len, 1)\n",
    "            tgt_mask = model.transformer.generate_square_subsequent_mask(seq_input.size(0)).to(device)\n",
    "            out = model.transformer.decoder(model.pos_encoder(model.embedding(seq_input)), memory, tgt_mask=tgt_mask)\n",
    "            logits = model.fc_out(out[-1, :])  # shape: (1, vocab_size)\n",
    "            log_probs = torch.log_softmax(logits, dim=-1)\n",
    "            topk_log_probs, topk_indices = torch.topk(log_probs, beam_width, dim=-1)\n",
    "            for j in range(beam_width):\n",
    "                candidate = topk_indices[0, j].unsqueeze(0)  # shape: (1,)\n",
    "                candidate_log_prob = topk_log_probs[0, j].item()\n",
    "                new_seq = torch.cat([seq, candidate], dim=0)\n",
    "                new_score = score + candidate_log_prob\n",
    "                new_beam.append((new_seq, new_score))\n",
    "        beam = sorted(new_beam, key=lambda x: x[1], reverse=True)[:beam_width]\n",
    "        if all(seq[-1].item() == end_token_id for seq, _ in beam):\n",
    "            break\n",
    "    best_seq, best_score = beam[0]\n",
    "    return best_seq.squeeze().tolist()\n",
    "\n",
    "def generate_translations_beam(model, sp, src_sentences, beam_width=3):\n",
    "    \"\"\"\n",
    "    Generate translations for a list of source sentences using beam search decoding.\n",
    "    Returns a list of translated sentences (strings).\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    translations = []\n",
    "    for sentence in src_sentences:\n",
    "        src_ids = sp.encode_as_ids(sentence.strip())\n",
    "        src_tensor = torch.tensor(src_ids, dtype=torch.long).unsqueeze(1)\n",
    "        translation_ids = beam_search_decode(model, src_tensor, sp, beam_width=beam_width)\n",
    "        translation = sp.decode_ids(translation_ids if isinstance(translation_ids, list) else [translation_ids])\n",
    "        translations.append(translation)\n",
    "    return translations\n",
    "\n",
    "def load_test_data(src_path, ref_path):\n",
    "    \"\"\"\n",
    "    Load test data from text files.\n",
    "    Each file should have one sentence per line.\n",
    "    Returns two lists: source sentences and reference sentences.\n",
    "    \"\"\"\n",
    "    with open(src_path, 'r', encoding='utf-8') as f:\n",
    "        src_sentences = [line.strip() for line in f if line.strip()]\n",
    "    with open(ref_path, 'r', encoding='utf-8') as f:\n",
    "        ref_sentences = [line.strip() for line in f if line.strip()]\n",
    "    return src_sentences, ref_sentences\n",
    "\n",
    "def evaluate_model():\n",
    "    # Define file paths\n",
    "    test_src_path = \"eng_test_mini.txt\"    # File with English test sentences\n",
    "    test_ref_path = \"ben_test_mini.txt\"     # File with corresponding reference Bengali translations\n",
    "    sp_model_path = \"spm_model.model\"\n",
    "    model_path = \"multilingual_nmt_model.pt\"\n",
    "    \n",
    "    # Hyperparameters (must match training settings)\n",
    "    vocab_size = 32000\n",
    "    d_model = 300\n",
    "    nhead = 6\n",
    "    num_encoder_layers = 3\n",
    "    num_decoder_layers = 3\n",
    "    \n",
    "    # Load SentencePiece model\n",
    "    sp = spm.SentencePieceProcessor(model_file=sp_model_path)\n",
    "    \n",
    "    # Initialize and load the trained NMT model\n",
    "    model = MultilingualTransformer(vocab_size, d_model, nhead, num_encoder_layers, num_decoder_layers)\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model.to(device)\n",
    "    model.load_state_dict(torch.load(model_path, map_location=device))\n",
    "    logging.info(\"Trained model loaded from %s\", model_path)\n",
    "    \n",
    "    # Load test data\n",
    "    src_sentences, ref_sentences = load_test_data(test_src_path, test_ref_path)\n",
    "    logging.info(\"Loaded %d test sentences.\", len(src_sentences))\n",
    "    \n",
    "    # Generate translations using beam search\n",
    "    generated_translations = generate_translations_beam(model, sp, src_sentences, beam_width=3)\n",
    "    \n",
    "    # Save generated translations for qualitative analysis\n",
    "    with open(\"generated_translations_beam.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "        for src, trans in zip(src_sentences, generated_translations):\n",
    "            f.write(\"Source: \" + src + \"\\n\")\n",
    "            f.write(\"Translation: \" + trans + \"\\n\\n\")\n",
    "    logging.info(\"Generated translations saved to generated_translations_beam.txt\")\n",
    "    \n",
    "    # Compute BLEU score using sacreBLEU (sacreBLEU expects a list of reference lists)\n",
    "    bleu = sacrebleu.corpus_bleu(generated_translations, [ref_sentences])\n",
    "    logging.info(\"BLEU score: %.2f\", bleu.score)\n",
    "    print(\"BLEU score:\", bleu.score)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    evaluate_model() "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
